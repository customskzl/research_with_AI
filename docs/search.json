[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Empirical Research with AI",
    "section": "",
    "text": "这本书是使用 VScode 编辑器 + Quarto 插件编写，进而通过 Github pages 发布的。\n详情参见 Quarto books。 \n\n主页 | 课程 | 视频 | 推文 | 资料",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>简介</span>"
    ]
  },
  {
    "objectID": "body/00_py_with_AI_intro.html",
    "href": "body/00_py_with_AI_intro.html",
    "title": "2  借助 AI 写代码",
    "section": "",
    "text": "2.1 AI 工具",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>借助 AI 写代码</span>"
    ]
  },
  {
    "objectID": "body/00_py_with_AI_intro.html#ai-工具",
    "href": "body/00_py_with_AI_intro.html#ai-工具",
    "title": "2  借助 AI 写代码",
    "section": "",
    "text": "通用工具\n\nChatGPT - 由 OpenAI 开发的聊天机器人，基于 GPT-3.5 架构。可以用于编写代码、回答问题、生成文本等。\nChatGPT Plus - 付费版本的 ChatGPT，提供更快的响应时间和更高的可用性。\nDeepSeek - 国内访问顺畅，整体表现不错，但结果的稳定性欠佳。\n豆包 - 更适合文字编排、翻译等。\nkimi - 与豆包相当。\n\n编程工具 (这些都是 VScode 中的插件)\n\nGitHub Copilot: VScode 插件，可以在编写代码时提供实时建议和补全。首月免费，后续 $10/月。\nTabnine: 代码补全工具，支持多种编程语言。\nclint: 代码补全工具，支持多种编程语言。\nCodeium: 代码补全工具，支持多种编程语言。",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>借助 AI 写代码</span>"
    ]
  },
  {
    "objectID": "body/00_py_with_AI_intro.html#理念",
    "href": "body/00_py_with_AI_intro.html#理念",
    "title": "2  借助 AI 写代码",
    "section": "2.2 理念",
    "text": "2.2 理念\n\n自然语言编程 vs. 传统编程\n\n\n「自然语言编程」与 Python、C++ 等传统编程本质上都是向计算机发出指令，要求其执行特定操作\n区别在于：\n\n传统编程语言（如 Python、C++）有严格的语法和结构\n自然语言编程则用人类语言（如中文、英文）描述操作\n\n\n\n思维方式与沟通能力\n\n\n初学时，自然语言编程似乎更简单\n真正发挥其潜力，关键在于思维方式和沟通方式（如何提问）\n学习曲线很陡峭：\n\n知识广度：你要知道很多东西以及他们的关联，才能提出好的问题\n知识深度：基本概念、核心理论、核心算法\n逻辑思维：界定问题、拆解问题、追问（横向 v.s. 纵向）\n语言表达：简洁、准确、清晰\n\n\n\n最核心的理念转变\n\n\n提示词 = 自然语言的”代码”\n写好提示词，就像写好 Python/C++ 代码一样重要\n许多高校已开设「提示词工程」课程，「Prompt 工程师」将成为热门职业\n\n\n推荐学习资料\n\n\nPrompt Engineering Guide\n吴恩达老师的 ChatGPT Prompt Engineering for Developers",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>借助 AI 写代码</span>"
    ]
  },
  {
    "objectID": "body/00_py_with_AI_intro.html#提示词",
    "href": "body/00_py_with_AI_intro.html#提示词",
    "title": "2  借助 AI 写代码",
    "section": "2.3 提示词",
    "text": "2.3 提示词\n\nTips\n\n\n先粗后细 e.g. 生成讲义 v.s 先细后粗\n顺藤摸瓜-迁移 e.g. 各种抽样方法\n虚构角色 e.g. 你是一个资深的英文经济学期刊的编辑 → 推文\n\n\n收集整理自己的提示词\n\n\nChatGPT Prompting Cheat Sheet\nThe Complete ChatGPT Cheat Sheet 2025!\nPrompt工作手册 - 方法篇",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>借助 AI 写代码</span>"
    ]
  },
  {
    "objectID": "body/00_py_with_AI_intro.html#我的使用经验",
    "href": "body/00_py_with_AI_intro.html#我的使用经验",
    "title": "2  借助 AI 写代码",
    "section": "2.4 我的使用经验",
    "text": "2.4 我的使用经验\n\n我常用的提示词\n\n\nhttps://github.com/arlionn/UseChatGPT\nhttps://gitee.com/arlionn/UseChatGPT (码云版)\n\n\n借助 AI 找 IV：连玉君的实战经验\n\n\n帮我找 20 个 IV\n寻找 IV 的提示词如何写？\n\n\n2.4.1 实例：使用 AI 写一篇完整的论文推介\n\nDu-2024-EE-中文精要生成过程\n\n核心提示词： 参见 连玉君的 Prompts\n\n\n\n\n\n\n提示词\n\n\n\nPrompt 1:\n{先上传论文的 PDF 版本给 ChatGPT，然后输入以下提示词：}\n“写一篇论文推介，介绍附件中的论文。先列个提纲给我。”\nPrompt 2:\n分批次输出吧：\n\n计量模型的证明和详细推导过程可以省略，但要补充简单直白的语言来解释模型和参数的经济含义\n把数学符号和公式都采用 Latex 格式来写，以保证输出美观\n行内公式采用 $f=x$ 格式，单行公式采用 $$f=x$$ 格式\n所有括号都用半角模式，中英文混排注意加空格\n不要添加任何表情符号\n按 ‘## 1. xxx’，‘### 1.1 xxx’，‘#### xxx’(不编号) 的格式来分 Section, Subsection, Subsubsection\n参考文献格式：\n\nxxx, xxx, xxx. (2023). xxx. Journal of xxx, 1(1), 1-10. [Link](https://doi.org/{DOI}), [-PDF-](http://sci-hub.ren/{DOI}), [Google](&lt;https://scholar.google.com/scholar?q={Title of the Paper}&gt;).\n\n注意：每次生成答案时，都在首行按如下格式添加 label，以便我追问时定位：‘mylabel-01’，‘mylabel-02’，……\n\nPrompt 3:\n连续输出，中间无需停顿\nPrompt 4:\n详细介绍一下 4.4 模型四：部分线性函数系数面板模型（PLFC）中的模型设定和估计方法\nPrompt 5:\n补充一个 Subsection，添加如下内容： - 为没有任何非参数估计基础的读者解释一下样条基函数（Sieve Estimation） - 再补充一个 subsection，解释一下边际效应的置信区间是如何计算的",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>借助 AI 写代码</span>"
    ]
  },
  {
    "objectID": "body/00_py_with_AI_pic.html",
    "href": "body/00_py_with_AI_pic.html",
    "title": "3  Ansome Python + AI",
    "section": "",
    "text": "3.1 画一幅图\n有了 AI 的协助，我们可以在只掌握少量编程知识的情况下，用 Python 实现各种复杂的功能。本章例举一些实例，旨在改变大家对「编程」的认知。\n最终你会发现，Python 只是我们完成某些分析和研究任务的工具而已。AI 的作用是让我们使用「自然语言」来编程。此时的编程主要包括如下几个要点：\n我在 R for Data Science 书中看到了一幅不错的图形，想要绘制出来。这幅图不算复杂：\n于是，我把图片发给了 AI 工具，配上提示词：\n我把相同的问题同时发给了 ChatGPT，DeepSeek 和 豆包。最终，豆包给出的图形效果最好：\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.table import Table\nfrom matplotlib.patches import FancyArrowPatch\n\n# 原始数据\ndata1 = {\n    'id': ['A', 'B'],\n    'x_1': [1, 5],\n    'x_2': [2, 6],\n    'y_1': [3, 7],\n    'y_2': [4, 8]\n}\ndf1 = pd.DataFrame(data1)\n\n# 转换后数据\ndata2 = {\n    'id': ['A', 'A', 'B', 'B'],\n    'x': [1, 2, 5, 6],\n    'y': [3, 4, 7, 8],\n    'num': [1, 2, 1, 2]\n}\ndf2 = pd.DataFrame(data2)\n\n# 定义颜色映射\ncolor_mapping = {\n    'x_1': 'lightblue',\n    'x_2': 'plum',\n    'y_1': 'lightgreen',\n    'y_2': 'lightgreen',\n    'x': 'lightblue',\n    'y': 'lightgreen',\n    'num': ['orange', 'violet']\n}\n\n# 创建图形和子图\nfig, ax = plt.subplots(figsize=(10, 4))\nax.axis('off')\n\n# 绘制原始表格\n# 绘制原始表格\ntable1 = ax.table(\n    cellText=df1.values,\n    colLabels=df1.columns,\n    cellLoc='center',\n    loc='left',\n    bbox=[0, 0.3, 0.4, 0.6]\n)\nfor (row, col), cell in table1.get_celld().items():\n    if row == 0:\n        cell.set_text_props(weight='bold')\n    if row &gt; 0:\n        colname = df1.columns[col]\n        cell.set_facecolor(color_mapping.get(colname, 'white'))\n    else:\n        colname = df1.columns[col]\n        cell.set_facecolor(color_mapping.get(colname, 'white'))\n\n# 绘制箭头\n#ax.arrow(0.45, 0.6, 0.1, 0, head_width=0.03, head_length=0.05, fc='k', ec='k')\n\n# 绘制转换后表格\ntable2 = ax.table(\n    cellText=df2.values,\n    colLabels=df2.columns,\n    cellLoc='center',\n    loc='right',\n    bbox=[0.55, 0.3, 0.4, 0.6]\n)\nfor (row, col), cell in table2.get_celld().items():\n    if row == 0:\n        cell.set_text_props(weight='bold')\n    if row &gt; 0:\n        colname = df2.columns[col]\n        if colname == 'num':\n            cell.set_facecolor(color_mapping['num'][(row-1) % 2])\n        else:\n            cell.set_facecolor(color_mapping.get(colname, 'white'))\n    else:\n        colname = df2.columns[col]\n        if colname == 'num':\n            cell.set_facecolor(color_mapping['num'][0])\n        else:\n            cell.set_facecolor(color_mapping.get(colname, 'white'))\n            # 添加 FancyArrowPatch 箭头以美化转换效果\n\n            fancy_arrow = FancyArrowPatch(\n                (0.48, 0.6), (0.52, 0.6),\n                transform=fig.transFigure,\n                connectionstyle=\"arc3,rad=0\",\n                arrowstyle='-|&gt;',\n                linewidth=2,\n                color='black'\n            )\n            fig.patches.append(fancy_arrow)\nplt.show()",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ansome Python + AI</span>"
    ]
  },
  {
    "objectID": "body/00_py_with_AI_pic.html#画一幅图",
    "href": "body/00_py_with_AI_pic.html#画一幅图",
    "title": "3  Ansome Python + AI",
    "section": "",
    "text": "提示词\n\n\n\n写一段 Python 代码，制作类似的图形。\n请注意左右两侧图形的颜色块要一一对应，这是这幅图的关键。\n只保留中文注释，图中的文字用英文。\n不要显示 warning information",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ansome Python + AI</span>"
    ]
  },
  {
    "objectID": "body/01_1_install-Python-Anocanda.html",
    "href": "body/01_1_install-Python-Anocanda.html",
    "title": "4  Python：安装和环境配置",
    "section": "",
    "text": "4.1 安装 Anaconda\n对于初学者来讲，建议安装 Anaconda 套装。它是一个开源的 Python 发行版，集成了 Python 解释器、包管理器 Conda 和许多常用的科学计算和数据分析库（如 NumPy、Pandas、Matplotlib 等）。\n虽然 Anaconda 自带的编辑器 Jupyter Notebook 很好用，但如果你平时经常用 VScode 写东西，建议安装 VScode 作为编辑器。VScode 支持多种编程语言，可以安装各种插件来增强功能。对于编写 Python 代码而言，仅需安装 python 和 jupyter 插件就可以满足基本需求。\n网上有不少关于 Anaconda 和 VScode 的安装和配置的教程，如：\n根据我的使用经验，基本步骤总结如下：\n下面，我详细说明每个步骤的操作。",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python：安装和环境配置</span>"
    ]
  },
  {
    "objectID": "body/01_1_install-Python-Anocanda.html#安装-anaconda",
    "href": "body/01_1_install-Python-Anocanda.html#安装-anaconda",
    "title": "4  Python：安装和环境配置",
    "section": "",
    "text": "Note\n\n\n\n对于多数用户，只需看 Step 1-2 即可。\n\n\n\n下载 Anaconda（建议注册一个账号，若不注册，可以点击 Skip）。\n安装 Anaconda。安装 Anaconda 最重要的事情：\n\nSelect Installation Type 页面，建议选择 Just Me，然后点击 Next。\nChoose Install Location 页面，建议使用默认路径 C:\\Users\\用户名\\Anaconda3 作为 Anaconda 的安装路径，这样可以避免一些潜在的权限和路径问题。然而， 如果你的用户名中包含中文字符或空格，建议选择「自定义路径」，并选择一个英文路径，如 C:\\myProgram\\Anaconda3。\nAdvanced Installation Options 页面，确保同时勾选如下两个选项：\n\n[√] Add Anaconda to my PATH environment variable\n[√] Register Anaconda as my default Python 3.x\n\n\n详情参见：VSCode与Anaconda安装配置\n\n安装完成后，打开 Anaconda Navigator（在开始菜单或应用程序中找到它）。\n在 Anaconda Navigator 中，你可以创建和管理虚拟环境、安装包、启动 Jupyter Notebook 等。\n安装完成后，打开 Anaconda Prompt（命令行界面），输入以下命令检查安装是否成功：\nconda --version\n如果显示版本号，则表示安装成功。\n在 Anaconda Prompt 中输入以下命令更新 Conda 到最新版本：\nconda update conda\n创建一个新的虚拟环境（可选）：如果你想在一个独立的环境中工作，比如，你要同时使用 Python 3.8 和 Python 3.12，以便完成不同的项目，你可以创建一个新的虚拟环境。输入以下命令创建一个名为 myenv38 的虚拟环境，并安装 Python 3.8：\nconda create --name myenv38 python=3.8\n同理，如果你想使用 Python 3.12，你可以创建一个名为 myenv312 的虚拟环境，并安装 Python 3.12：\nconda create --name myenv312 python=3.12\n接下来，你可以激活特定的虚拟环境，比如 myenv38，输入以下命令：\nconda activate myenv38\n此时，若执行 canda list 命令，你会看到当前环境中安装的所有包和版本信息；而执行 canda install Stargazer, v = 2.1.1，则会在当前环境中安装 Stargazer 包的 2.1.1 版本。",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python：安装和环境配置</span>"
    ]
  },
  {
    "objectID": "body/01_1_install-Python-Anocanda.html#安装-vscode",
    "href": "body/01_1_install-Python-Anocanda.html#安装-vscode",
    "title": "4  Python：安装和环境配置",
    "section": "4.2 安装 VScode",
    "text": "4.2 安装 VScode\n\n下载 VScode（选择适合你操作系统的版本）。\n安装 VScode（双击下载的安装包，按照提示完成安装）。\n安装完成后，打开 VScode。\n在 VScode 中，你可以 安装各种插件 来增强功能，比如 Python、Jupyter 等。\n\n\n4.2.1 建议安装的 VScode 插件\n\n如果你不了解 VScode，可以先读一下 VScode编辑器。\n有关插件的安装和使用，参见：VScode插件：安装、配置和使用\n安装插件很容易：点击图中的 四个小方块 图标，在搜索框中填入插件名称，点击 Install 即可。\n\n\n\n4.2.1.1 Python 插件\n\nVScode：实用 Python 插件清单\n\n\nPython：必装，运行 Python 代码\nJupyter：必装，运行 Jupyter Notebook 文件\nPylance：代码补全和智能提示等功能\nData Wrangler：表格呈现\n编程助手 (选一个即可，否则可能会有冲突)\n\nGitHub Copilot (首月免费，后续每月 $10.0，使用 Visa 或 Master 信用卡付款)\nCodeium (Copilot 的替代品，Free，目前已经更名为 Windsurf Plugin)\nCline / Cline Chinese (Copilot 的替代品，Free)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n安装编程助手前，请确保能顺畅上网。\n建议预先申请一个 GitHub 账号，使用 Copilot 时，可以用 GitHub 账号进行关联和登录。\n我个人使用 Copilot，觉得效果还不错。Codeium 据说不错，甚至在 VScode 中的安装量比 Copilot 还要多。Cline 是最近新出的，大家也可以测试一下。\n\n\n\n\n\n4.2.1.2 Markdown 插件\n\nVScode：实用 Markdown 插件推荐\n\n\nMarkdown All in One\nMarkdown Preview Enhanced\nMarp (制作幻灯片)",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python：安装和环境配置</span>"
    ]
  },
  {
    "objectID": "body/01_1_install-Python-Anocanda.html#jupyter-notebook-的使用",
    "href": "body/01_1_install-Python-Anocanda.html#jupyter-notebook-的使用",
    "title": "4  Python：安装和环境配置",
    "section": "4.3 Jupyter Notebook 的使用",
    "text": "4.3 Jupyter Notebook 的使用\n\n4.3.1 简介\n\n\n\n\n\n\nNote\n\n\n\n\n这里介绍的 Jupyter Notebook 是指在 VScode 中使用的 Jupyter Notebook，而不是 Anaconda 自带的 Jupyter Notebook。VScode 中的 Jupyter Notebook 更加灵活和强大，支持多种编程语言和插件。\n在 VScode 中安装了 Jupyter 插件后，就已经完成了 Jupyter Notebook 的安装和配置。\n\n\n\n顾名思义，Jupyter Notebook 就是一个文本编辑器。如果你使用过 Stata，那么它类似于 Stata 的 Do 文件；如果你使用过 RStudio，那么它类似于 RStudio 的 R Markdown 文件。Jupyter Notebook 的一个重要特点就是它是一个交互式的计算环境，允许你在一个文档中编写和运行代码、可视化数据、撰写文本和公式等。\n简言之，你可以把你的分析目的 (文本块)，分析过程 (代码块)，分析结果 (表格或图形) 都整合到一个文档中，形成一个完整的分析报告。\n\n\n\n4.3.2 安装\n在 VSCode 中使用 Jupyter Notebook，你需要安装以下软件：\n\nPython：Jupyter Notebook 是基于 Python 的，因此你需要安装 Python。你可以从 Python 官网 下载并安装最新版本的 Python。\nVSCode：你需要安装 Visual Studio Code。你可以从 VSCode 官网 下载并安装最新版本的 VSCode。\nJupyter 扩展：在 VSCode 中使用 Jupyter Notebook，你需要安装 Jupyter 扩展。你可以在 VSCode 的扩展市场中搜索 “Jupyter” 并安装它。\nPython 扩展：你还需要安装 Python 扩展。你可以在 VSCode 的扩展市场中搜索 “Python” 并安装它。\n\n\n\n4.3.3 新建项目文件结构\n\n本地新建一个文件夹，如 D:/FinData/Proj_Maotai\n(推荐) 在该文件夹下新建如下几个子文件夹，用于分类存放文件\n\n[./data]：存放 .csv、.xlsx 等数据文件\n[./doc]：存放说明文档、报告等文件\n[./codes]：存放代码文件，包括自编函数、模块等\n[./output]：存放输出文件，包括图表、结果等\n此外，建议在该文件夹下新建一个 README.md 文件，对项目进行说明 (如项目背景、目标、数据来源、程序的执行顺序和方法等)。\n\n\n\n\n20250516012952\n\n\n\n\n4.3.3.1 新建 Jupyter Notebook 文件 (.ipynb)\n\n打开 VScode，依次点击 文件 -&gt; 打开文件夹，选择 D:/FinData/Proj_Maotai 文件夹。\n右击该文件夹，选择 New File (亦可直接点击下图中的 New File 图标)，新建一个文件，名称为 data_clean.ipynb。注意：一定要以 .ipynb 结尾，否则无法用 Jupyter Notebook 打开，也无法添加 Python 代码块。\n\n你也可以使用命令行方式新建一个 .ipynb 文件：按快捷键 Ctrl+Shift+P &gt;&gt; 输入或选择 Create: New Jupyter Notebook 会打开一个名为 Untitled-1.ipynb 的文件，输入内容后另存到目标文件夹即可。\n\n\n完成上述设定后，VScode 的文件目录结构如下：\n# 目录结构\nD:/FinData/Proj_Maotai\n├── codes\n├── data\n├── doc\n├── output\n├── README.md\n└── data_clean.ipynb\n\n\n4.3.3.2 借助 AI 工具自动创建项目文档 (备选)\n\nChatGPT 对话过程\n\n如果觉得手动创建文件夹麻烦，可以借助 ChatGPT 等 AI 工具，自动创建项目文档。下面是我编写的一个示例提示词 (你只需要指定目标文件夹路径即可，即使这个文件夹不存在也没有关系)：\n\n请帮我在 Windows 系统上为一个数据分析项目生成标准的项目结构。 项目的根目录是 D:/FinData/Proj_Maotai，请在该路径下创建以下结构：\nD:/FinData/Proj_Maotai\n├── codes             # 用于存放脚本代码\n├── data              # 原始数据或处理后的数据\n├── doc               # 项目文档与说明资料\n├── output            # 图表、模型结果、分析报告等输出文件\n├── README.md         # 项目说明文件，简要介绍项目目的与结构\n├── 01_data_clean.ipynb       # 数据清洗与预处理\n├── 02_EDA.ipynb              # 探索性数据分析\n└── 03_regression.ipynb       # 回归分析与建模\n请提供 Python 代码，以便我在写 Jupyter Notebook 中运行。 请确保必要的 .ipynb 文件和 README.md 都被自动生成。如果文件已存在，请保留原文件不覆盖。 你可以默认这些 .ipynb 文件为空白，也可以加一两行注释。\n\n经过测试，ChatGPT 和 豆包生成的代码都可以一次性顺利执行：\n\nChatGPT 对话过程\n豆包对话过程\n\n\n\n4.3.3.3 采用插件创建项目模板\n除了上述方法，你也可以在 VScode 中搜索 Project Templates，安装对应的插件来创建项目模板。安装完成后，点击左侧的 Project Templates 图标，选择 Create New Project，然后选择你需要的模板即可。\n\n该插件提供了多种项目模板，包括数据分析、机器学习、Web 开发等，可以酌情选择。\n该插件也支持自定义模板，具体操作请参考插件的说明文档。\n\n不过，如果项目不是很复杂，建议还是手动创建项目文件夹，这样更灵活，也更容易管理。\n\n\n\n4.3.4 使用 Jupyter Notebook\nStep 1：选择解释器。初次打开 data_clean.ipynb 文件时，可能需要按下图方式设定 Python 解释器，以便随后用 python.exe 来运行新增的代码块。\n\nStep 2：添加 Markdown 文本块。点击上图中的 + Markdown 按钮，添加一个 Markdown 文本块。可以用 Markdown 语法撰写文档说明。\n\n预览：点击图中的 √ 或按快捷键 Esc；\n编辑：在预览 双击鼠标 区可重新计入编辑模式。\n\n\nStep 3a： 让 AI 自动生成代码(可选)：如果你已经在 VScode 中安装了插件 Github Copilot，可以点击图中的 Generate 按钮，输入提示词 (比如，生成随机数 x~N(0,1)，N=100，绘制直方图+密度函数图, 英文标题)，敲回车，或点击下图中的右箭头，Copilot 会自动生成代码。如果代码块右下角显示的不是 python，可以单击之，选择 python，将次代码块标记为 Python 语言。\n\n\nStep 3b： 自行添加代码块。点击上图中的 + Code 按钮，添加一个代码块。可以在代码块中输入 Python 代码。\n\n运行代码块：点击上图中的 ▶ 按钮，或按快捷键 Ctrl + Enter；\n\n\nStep 4： 修改和调试代码。你可以酌情修改代码。\n\n若出现大量警告信息或程序无法运行，可以选中全部代码或被标注了红色波浪线代码，点击下图中的黄色星星按钮，选择 Fix using copilot，多数情况下都能自动修复。\n若相对可以正常运行的代码做一些优化，则可以选中代码后，点击黄色星星，选择 Modify using copilot，然后输入提示词。\n\n\n\n\n20250516021357\n\n\n\n4.3.4.1 界面和主要功能\n在上文中，为了突出重点，我没有让 VScode 编辑器全屏。事实上，全屏后，你会发现 Jupyter Notebook 提供了一组功能强大的工具栏和菜单栏，帮助你更好地编写和运行代码。下面是 Jupyter Notebook 的主要界面元素：\n\n多数菜单的功能都是比较直观的，下面我简单介绍几个常用的功能：\n\nView data：查看数据和概要信息。注意：需要安装 Data Wrangler 插件才具有此功能。\n...：最右侧的三个点，点击后可以选择 Export，将当前 Notebook 导出为 PDF、HTML、Markdown 等格式；还有预览 (Preview) 等功能 (快捷键 Esc)。\n\n\n\n\n4.3.5 在文档间复制 cell\n\n\n4.3.6 单个 cell 的复制和粘贴\n在 Jupyter Notebook 中，你可以通过以下方式在不同的 Notebook 文档之间复制 cell： 1. 打开源 Notebook 文档，选中要复制的 cell。 2. 使用快捷键 Ctrl+C 复制选中的 cell；亦可单击三个点图标，选择 Copy Cell。 3. 打开目标 Notebook 文档，将光标定位到要粘贴的位置。 4. 使用快捷键 Ctrl+V 粘贴 cell；亦可单击三个点图标，选择 Paste Cell。\n\n4.3.6.1 多个 cell 的复制和粘贴\n\n在 VS Code 的资源管理器（Explorer）中，分别打开需要操作的两个 Notebook 文件（如 A.ipynb 和 B.ipynb）。\n\n如需分屏显示，可拖动标签，将两个文档左右排列，方便拖拽和对比操作。\n\n在源 Notebook（如 A.ipynb）中，按住 Ctrl（Mac 下为 Command）逐个点击左侧 cell 区域，可多选不连续的 cell；或先点击一个 cell，再按住 Shift 点击另一个 cell，选择一段连续的 cell。\n\n多选后，cell 区域会高亮显示，表示已被选中。\n\n按 Ctrl+C（Command+C）或右键点击选中区域，选择“Copy Cell”。\n切换到目标 Notebook（如 B.ipynb），点击你希望插入的位置。\n按 Ctrl+V（Command+V）或右键选择“Paste Cell”，即可将所有选中 cell 批量粘贴到目标位置。",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python：安装和环境配置</span>"
    ]
  },
  {
    "objectID": "body/01_1_install-Python-Anocanda.html#导出和文档转换",
    "href": "body/01_1_install-Python-Anocanda.html#导出和文档转换",
    "title": "4  Python：安装和环境配置",
    "section": "4.4 导出和文档转换",
    "text": "4.4 导出和文档转换\n在 VScode 中，你可以将 Jupyter Notebook 导出为多种格式，包括 HTML、PDF 和 Markdown。你可以在菜单栏中选择 文件 -&gt; 导出为，然后选择你想要的格式。当然，还需要安装一些额外的工具来支持这些导出功能，主要包括 Pandoc 和 TeX Live。\n\n4.4.1 安装 Pandoc（用于文档格式转换）\nJupyter 和 Quarto 在导出 Markdown、Word、PDF 时都会调用 Pandoc。\n\n推荐安装方式（适用于 Anaconda 用户）：\nconda install -c conda-forge pandoc\n或者前往官网下载安装包（Windows/macOS/Linux 均支持）： https://pandoc.org/install\n安装完成后，你可以在终端中输入以下命令测试是否成功：\npandoc --version\n\n\n\n4.4.2 安装 TeX Live（用于导出 PDF）\nJupyter 和 Quarto 的 PDF 导出依赖 LaTeX 引擎，如 pdflatex 或 xelatex。推荐安装完整版本的 TeX Live。\n\nWindows 用户请访问官网下载器： https://tug.org/texlive/windows.html\n安装过程中可选择 “完整版” 或 “Typical full installation”；\n安装完成后，务必重启 VS Code 和终端；\n然后在终端中测试：\npdflatex --version\n\n\n\n4.4.3 如何打开终端？\n很多初学者不清楚“在终端中输入命令”具体指什么。你可以按以下方式打开终端：\n\n方式一（推荐）： 在 VS Code 中按下快捷键 Ctrl + ~（就是数字 1 左侧的波浪线），即可打开内置终端。\n方式二： 从系统菜单中打开：\n\nWindows：点击开始 → 输入 cmd 或 Anaconda Prompt 或 PowerShell；\nmacOS：打开 Launchpad → 搜索「终端（Terminal）」；\nLinux：按下 Ctrl + Alt + T 打开终端窗口。\n\n\n终端窗口中，你可以输入如 conda install、pandoc --version 等命令，按下回车运行。\n\n\n4.4.4 安装完成后即可使用以下方式导出 PDF\n\n在 VS Code 中点击右上角「导出（Export）」按钮；\n或在终端中运行：\njupyter nbconvert your_notebook.ipynb --to pdf\n或使用 Quarto（如果你已安装）：\nquarto render your_notebook.ipynb --to pdf\n\n完成上述安装和设定后，即可顺利将 Notebook 文档导出为 PDF、HTML、Word 等多种格式，满足教学、写作和发布的常见需求。\n\n\n4.4.5 高阶用法：使用 Quarto 导出漂亮 PDF 或制作电子书网站\n如果你希望导出的文档更美观、格式更统一，或者希望将多个 Jupyter Notebook 或 Markdown 文档整合成一个电子书（book）或教学网站（website），可以考虑使用 Quarto。\nQuarto 是 RStudio 团队开发的下一代科学与技术出版平台，同时支持 .ipynb、.qmd、.md 文件的混合编写和发布。常见功能包括：\n\n一键导出 排版美观的 PDF 文档（支持标题、目录、脚注、交叉引用、引用文献等）；\n创建结构化的电子书（Quarto Book），适合撰写讲义、教程和学术教材；\n部署交互式教学网站（Quarto Website），可发布为静态网页托管在 GitHub Pages 上。\n\n\n4.4.5.1 示例命令：\n将 .ipynb 或 .qmd 导出为 PDF：\nquarto render your_notebook.ipynb --to pdf\n将多个文档组织为电子书结构（在 _quarto.yml 中设置章节）：\nquarto render\n\n\n4.4.5.2 安装方式：\n\n可从官网下载安装：https://quarto.org/download\n安装完成后，在终端中测试：\nquarto --version\n\nQuarto 完全兼容你现有的 VS Code 工作流，安装 Quarto 插件后即可直接在 VS Code 中预览、渲染和发布。对于希望构建专业文档的用户，Quarto 是目前最值得推荐的方案。",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python：安装和环境配置</span>"
    ]
  },
  {
    "objectID": "body/01_2_install_FAQs.html",
    "href": "body/01_2_install_FAQs.html",
    "title": "5  Python 安装常见问题",
    "section": "",
    "text": "5.1 安装了 Anaconda 还需要安装 Python 3.13 吗？\n答：不需要！ 而且，强烈反对。Anaconda 是一个完整的 Python 科学计算发行版，安装时会自动集成 Python 解释器和常用扩展包 (如 NumPy、Pandas 等)，并通过 Conda 环境管理器提供灵活的版本控制。\n若额外安装官网 Python，可能引发以下问题：\n建议通过 Conda 统一管理 Python 环境，既能自由切换不同版本（如 conda create -n py312 python=3.12），又能确保依赖隔离和生态兼容性，避免因混合安装导致的技术债。",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/01_2_install_FAQs.html#安装了-anaconda-还需要安装-python-3.13-吗",
    "href": "body/01_2_install_FAQs.html#安装了-anaconda-还需要安装-python-3.13-吗",
    "title": "5  Python 安装常见问题",
    "section": "",
    "text": "版本混乱：系统环境变量可能优先指向非 Anaconda 的 Python，导致命令行调用错误的解释器版本；\n\n依赖冲突：Conda 和 pip 混合管理包时，依赖项版本容易冲突（例如 TensorFlow 或 PyTorch 的兼容性问题）；\n\n环境隔离失效：全局安装的 Python 可能干扰 Conda 创建的独立环境，破坏项目隔离性；\n\n维护困难：需手动协调多个 Python 实例的更新与兼容性，增加运维复杂度。",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/01_2_install_FAQs.html#安装-anaconda-时需要注意什么",
    "href": "body/01_2_install_FAQs.html#安装-anaconda-时需要注意什么",
    "title": "5  Python 安装常见问题",
    "section": "5.2 安装 Anaconda 时需要注意什么？",
    "text": "5.2 安装 Anaconda 时需要注意什么？\n\nChoose Install Location 页面，建议使用默认路径 C:\\Users\\用户名\\Anaconda3 作为 Anaconda 的安装路径，这样可以避免一些潜在的权限和路径问题。注意： 如果你的用户名中包含中文字符或空格，建议选择「自定义路径」，并选择一个英文路径，例如 C:\\PromgramFile\\Anaconda3。\nAdvanced Installation Options 页面，确保同时勾选如下两个选项：\n\n[√] Add Anaconda to my PATH environment variable\n[√] Register Anaconda as my default Python 3.x\n\n\n详情参见：VSCode与Anaconda安装配置",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/01_2_install_FAQs.html#默认解释器的路径",
    "href": "body/01_2_install_FAQs.html#默认解释器的路径",
    "title": "5  Python 安装常见问题",
    "section": "5.3 默认解释器的路径",
    "text": "5.3 默认解释器的路径\n问题描述：每次在 VScode 的 Jupyter 中运行代码时，都会弹出一个窗口询问选择解释器的路径。\n\n原因 1：VScode 设置问题：VScode 在运行 Jupyter Notebook 时需要指定 Python 解释器的路径，如果没有设置默认解释器，就会弹出这个窗口。 environment variable`，导致 VScode 无法找到 Python 解释器的路径。\n\n解决方法：在 VScode 中，按下 Ctrl + Shift + P，输入Python: Select Interpreter\\，选择你想要的 Python 解释器路径即可。\n\n原因 2：Anaconda 安装时的配置有问题。\n在安装 Anaconda 时，在 Advanced Installation Options 页面，你可能没有勾选 Add Anaconda to my PATH environment variable，导致 VScode 无法找到 Python 解释器的路径。\n\n解决方案：将 Anaconda 的安装路径添加到系统的环境变量中。具体步骤如下：\n\n在 Windows 中，右键点击 “此电脑”，选择 “属性”。\n依次点击 高级系统设置 &gt;&gt; 环境变量\n在 “系统变量” 中找到 Path，点击 “编辑”。\n点击 “新建”，添加 Anaconda 的安装路径，例如 C:\\Users\\用户名\\Anaconda3 和 C:\\Users\\用户名\\Anaconda3\\Scripts。\n点击“确定”，保存设置。\n\n\n\n\n20250513211532",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/01_2_install_FAQs.html#pip-install-在哪里执行",
    "href": "body/01_2_install_FAQs.html#pip-install-在哪里执行",
    "title": "5  Python 安装常见问题",
    "section": "5.4 pip install 在哪里执行？",
    "text": "5.4 pip install 在哪里执行？\n答：pip install pkg_name 通常在命令行界面中执行。不同操作系统打开命令行界面的方式略有不同：\n\nWindows 系统：按下 Win + R 键，输入 cmd，然后按下 Enter 键。\nMac 系统：按下 Command + Space 键，输入 Terminal，然后按下 Enter 键。\nVSCode 中：按下 Ctrl + Shift + P，输入 Terminal: Create New Terminal，然后按下 Enter 键。\n更快捷的方式 (VScode 中)：\n\nWindows：按下 Ctrl + ~ 键 (~ 在 Esc 键下方)。\nMac：按下 Command + Shift + ~ 键。",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/01_2_install_FAQs.html#什么是-vscode-的-terminal",
    "href": "body/01_2_install_FAQs.html#什么是-vscode-的-terminal",
    "title": "5  Python 安装常见问题",
    "section": "5.5 什么是 VScode 的 terminal？",
    "text": "5.5 什么是 VScode 的 terminal？\n答：VScode 的 terminal 是一个集成的命令行界面，允许用户在 VScode 编辑器中直接执行命令行操作。它支持多种 Shell，如 PowerShell、Bash 和 Command Prompt 等。用户可以在 terminal 中运行 Python 脚本、安装包、管理版本控制等操作，而无需切换到外部命令行窗口。\n\n\n\n20250513225516\n\n\n开启方法：\n\n在 VScode 中，按下 Ctrl + Shift + P，输入 Terminal: Create New Terminal，然后按下 Enter 键。\n更快捷的方式：\n\nWindows：按下 Ctrl + ~ 键（~ 在 Esc 键下方）。\nMac：按下 Command + Shift + ~ 键。\n\n\n用途：\n\n执行命令行操作，如安装 Python 包（pip install pkg_name）、运行脚本等。\n运行 Git 命令进行版本控制：\n\ngit clone：克隆远程仓库到本地。\ngit add：将更改添加到暂存区。\ngit commit：提交更改到本地仓库。\ngit push：将本地提交推送到远程仓库。\n\n管理虚拟环境和依赖项。\n调试和测试代码。\n查看系统信息和环境变量等。",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/01_2_install_FAQs.html#conda-install-与-pip-install-有何差别",
    "href": "body/01_2_install_FAQs.html#conda-install-与-pip-install-有何差别",
    "title": "5  Python 安装常见问题",
    "section": "5.6 Conda install 与 pip install 有何差别？",
    "text": "5.6 Conda install 与 pip install 有何差别？\n相似点：都是 Python 的包管理工具；都能安装 Python 库和包，并自动处理依赖关系。\n主要区别：\n\n包来源不同：\n\nConda：主要从 Anaconda 官方仓库（例如 Anaconda Cloud）安装包，并且可以安装 Python 之外的其他工具和库（例如 R 语言的库）。它支持更广泛的语言和工具。\npip：从 Python 官方包索引（PyPI）安装 Python 包，通常只用于 Python 相关的包。\n\n虚拟环境管理：\n\nConda：不仅可以管理 Python 包，还可以管理虚拟环境。在 Conda 环境中，你可以控制整个环境的配置，包括 Python 版本和依赖的库。\npip：仅管理 Python 包，不负责环境的管理。虽然 virtualenv 或 venv 可以配合 pip 使用来管理虚拟环境，但这需要额外的配置。\n\n依赖解决：\n\nConda：在安装包时，Conda 会自动解决依赖冲突，确保所有包和环境的兼容性。\npip：通常会安装指定的包，但不总是自动解决复杂的依赖关系。如果发生依赖冲突，pip 可能不会提醒用户。\n\n适用场景：\n\nConda：适用于需要管理多个工具和语言环境（如 Python、R）的情况，尤其在数据科学、机器学习等领域中，Conda 更加常用。\npip：适用于单纯的 Python 项目，尤其是当你不需要其他语言支持时，pip 更简便。\n\n\n\n总结\n\nAnaconda 是一个包含 Conda 的大套件，提供了用于数据分析和科学计算的工具。\nConda 是 Anaconda 的包和环境管理工具，可以管理 Python 以及其他语言的包。\npip 是 Python 的官方包管理工具，通常用于管理 Python 包，适用于纯 Python 项目。",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/01_2_install_FAQs.html#conda-和-anaconda-有何关系",
    "href": "body/01_2_install_FAQs.html#conda-和-anaconda-有何关系",
    "title": "5  Python 安装常见问题",
    "section": "5.7 Conda 和 Anaconda 有何关系？",
    "text": "5.7 Conda 和 Anaconda 有何关系？\nAnaconda 是一个开源的 Python 发行版，旨在简化 Python 和 R 编程语言中数据科学、机器学习和科学计算的使用。Anaconda 提供了包括 Python 和 R 语言在内的各种开源库以及管理和部署环境所需的工具。\nConda 是 Anaconda 的一个组件，它是一个包管理器和环境管理器。Conda 负责安装、更新、卸载包和管理虚拟环境。实际上，Conda 不仅仅可以用来管理 Python 包，还可以管理其他语言（例如 R 和 Ruby）的包。\n总结：Anaconda 是一个包含 Conda 的完整发行版，Conda 是其中的包管理工具。\n\nConda 的用途\n\n包管理：Conda 用于安装、升级和卸载 Python 包以及其他语言的包。它能够管理并解决依赖问题，确保你安装的包兼容且稳定。\n环境管理：Conda 允许创建和管理虚拟环境。通过虚拟环境，你可以在同一台机器上管理多个不同版本的 Python 和包，而不会发生冲突。例如，你可以为一个项目使用 Python 3.7，为另一个项目使用 Python 3.9，并且确保每个项目有独立的依赖。",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/01_3_markdown.html",
    "href": "body/01_3_markdown.html",
    "title": "6  Markdown",
    "section": "",
    "text": "6.1 何谓 Markdown？\nMarkdown 是一种轻量级的标记语言，允许你使用易读易写的纯文本格式编写文档，然后转换成结构化的 HTML, PDF, Word 等多种格式的文档。Markdown 语法简单易学，适合用来撰写笔记、文档、幻灯片等。\n大家在网上看到的很多博客文章，程序说明文档，甚至是在线书籍 (Python for Data Analysis, 3E)，都是用 Markdown 写的。",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "body/01_3_markdown.html#语法速览",
    "href": "body/01_3_markdown.html#语法速览",
    "title": "6  Markdown",
    "section": "6.2 语法速览",
    "text": "6.2 语法速览\n\n\n\n\n\n\nFigure 6.1: Markdown 语法对照\n\n\n\n你可以在如下网站按部就班地学习 Markdown 的基本用法，大概五分钟后你就可以掌握常用 Markdown 语法规则了：\n\nhttps://www.markdowntutorial.com/zh-cn",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "body/01_3_markdown.html#markdown-基本语法",
    "href": "body/01_3_markdown.html#markdown-基本语法",
    "title": "6  Markdown",
    "section": "6.3 Markdown 基本语法",
    "text": "6.3 Markdown 基本语法\n\nSource: markdownguide.org/cheat-sheet/\n\n这份 Markdown 备忘单介绍了常用的 Markdown 语法。为了便于您快速了解基本的语法规则，这里略去了很多细节信息，详情参见：基础语法 和 扩展语法。\n以下是 John Gruber 原始设计文档中列出的基本元素，所有 Markdown 应用程序都支持这些元素。\n\n\n\n\n\n\n\n元素\nMarkdown 语法\n\n\n\n\n标题\n# 一级标题## 二级标题### 三级标题\n\n\n粗体\n**粗体文本**\n\n\n斜体\n*斜体文本*\n\n\n引用块\n&gt; 引用内容\n\n\n有序列表\n1. 第一项2. 第二项3. 第三项\n\n\n无序列表\n- 第一项  -  第一条- 第二项- 第三项\n\n\n代码高亮显示\n`代码` (`xtreg` → xtreg)\n\n\n水平线\n---\n\n\n链接\n[连享会主页](https://www.lianxh.cn)\n\n\n图片\n![图片标题](/Fig/image.jpg) 或 ![](图片网址)\n\n\n\n\n6.3.1 表格\n| 命令    | 范例                 |\n| :------ | :------------------- |\n| xtreg   | `xtreg y x, fe`      |\n| reghdfe | `reghdfe y x, a(id)` |\n\n\n\n命令\n范例\n\n\n\n\nxtreg\nxtreg y x, fe\n\n\nreghdfe\nreghdfe y x, a(id)\n\n\n\n\n\n6.3.2 数学公式\n\n单行数学公式用 $$ 符号包围起来；\n行内数学公式用 $ 符号包围起来；\n包围符号内侧不要有空格，否则在有些 Markdown 编辑器中无法正常显示公式\n\n正确：$y = f(x)$\n错误：$ y = f(x) $ 或 $y = f(x) $\n\n有关 LaTeX 数学公式的语法和工具，参见：\n\nMarkdown常用LaTex数学公式\n神器-数学公式识别工具-mathpix\n\n\n模型设定为：\n\n$$y_{it} = \\alpha_i + x_{it}\\beta + u_{it}$$\n\n其中，$y_{it}$ 为被解释变量，$\\alpha_i$ 为个体效应。\n模型设定为：\n\\[y_{it} = \\alpha_i + x_{it}\\beta + u_{it}\\]\n其中，\\(y_{it}\\) 为被解释变量，\\(\\alpha_i\\) 为个体效应。\n\n\n6.3.3 代码块\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6]\n})\n```\n\n```stata\nsysuse \"auto.dta\", clear\nregress mpg weight\ndisplay \"Results: \" 2 + 3\n```\n渲染效果：\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6]\n})\nsysuse \"auto.dta\", clear\nregress mpg weight\ndisplay \"Results: \" 2 + 3\n\n\n6.3.4 扩展阅读\n\n初虹, 2024, 让「记录」变得简单：Markdown使用详解, 连享会 No.1456.\n初虹, 2021, 学术论文写作新武器：Markdown-上篇, 连享会 No.603.\n初虹, 2021, 学术论文写作新武器：Markdown-下篇, 连享会 No.604.\n初虹, 2021, 学术论文写作新武器：Markdown-中篇, 连享会 No.605.\n连玉君, 2024, VScode插件：安装、配置和使用, 连享会 No.1490.\n连玉君, 2024, VScode：实用 Markdown 插件推荐, 连享会 No.1390.",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "body/01_3_markdown.html#marp-幻灯片",
    "href": "body/01_3_markdown.html#marp-幻灯片",
    "title": "6  Markdown",
    "section": "6.4 Marp 幻灯片",
    "text": "6.4 Marp 幻灯片\n在 VScode 中安装 Marp 插件后，你可以使用 Markdown 语法来创建幻灯片。使用 Marp 最大的好处是你可以专注于内容，而不必担心幻灯片的格式和样式。Marp 会自动将你的 Markdown 文档转换为美观的幻灯片。\n\n\n6.4.1 安装\n1. VScode 编辑器\n2. Marp 插件 - 在 VScode 中打开扩展市场，搜索 Marp for VS Code，点击安装。\n3. 如需使用 PDF 导出功能，还需要安装 Marp CLI： - 打开终端 (TERMINAL)：在 VScode 中，点击左侧活动栏的终端图标，或使用快捷键 Ctrl + ~（反引号）打开终端。 - 在终端中运行以下命令安装 Marp CLI： bash      npm install -g @marp-team/marp-cli\n扩展阅读：\n\n连玉君, 2024, VScode插件：安装、配置和使用, 连享会 No.1490.\n连玉君, 2024, VScode：实用 Markdown 插件推荐, 连享会 No.1390.\n\n\n\n6.4.2 模版 1：最基本的设定\n以下是一个简单的 Marp 幻灯片模板，你只需要新建一个 .md 文件，输入如下内容，然后在 VScode 中打开该文件即可：\n---\nmarp: true\n---\n\n# 幻灯片标题\n\n---\n\n## 第一页幻灯片\n\n- xxx\n- xxx\n\n---\n\n## 第二页幻灯片\n\n- xxx\n- xxx\n\n\n6.4.3 模版 2：更多的设定\n该模板的主要功能包括：\n\n幻灯片的标题、作者、页码、脚注\n幻灯片的字号\n标题的颜色和页面背景颜色\n\n---\nmarp: true\nsize: 16:9      # 宽版：4:3\npaginate: true  # 显示页码\nfooter: '脚注文本或 [xxx](URL)'\n---\n\n&lt;style&gt;\n/*一级标题局中*/\nsection.lead h1 {\n  text-align: center; /*其他参数：left, right*/\n}\nsection {\n  font-size: 22px;      /* 正文字号 */\n}\nh1 {\n  color: blackyellow;   /* 标题的颜色 */\n  /*font-size: 28px; */ /* 标题的字号, 其它标题也可以这样修改 */\n}\nh2 {\n  color: green;\n}\nh3 {\n  color: darkblue;\n}\n\n/* 右下角添加页码 */\nsection::after {\n  content: attr(data-marpit-pagination) '/' attr(data-marpit-pagination-total); \n}\nheader,\nfooter {\n  position: absolute;\n  left: 50px;\n  right: 50px;\n  height: 25px;\n}\n&lt;/style&gt;\n\n&lt;!--顶部文字--&gt;\n[lianxh.cn](https://www.lianxh.cn/news/46917f1076104.html) \n\n&lt;br&gt;\n\n&lt;!--封面图片--&gt;\n![bg right:50% w:400 brightness:. sepia:50%](https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/20220722114227.png) \n\n&lt;!--幻灯片标题--&gt;\n# Marp 参数设置\n\n&lt;br&gt;\n&lt;br&gt;\n\n&lt;!--作者信息--&gt;\n[连玉君](https://www.lianxh.cn) (中山大学)\narlionn@163.com\n\n&lt;br&gt;\n---\n&lt;!-- backgroundColor: #FFFFF9 --&gt;\n## 第一页幻灯片\n\n- 背景是淡黄色的，可以根据需要修改颜色\n\n---\n&lt;!-- backgroundColor:white --&gt;\n## 第二页幻灯片\n\n- 背景是纯白色\n- 下面的图片在右侧，占页面 60% 的宽度\n\n![bg right:60% w:800](图片网址)\n详情参见：\n\n宋森安, 2021, 用Markdown制作幻灯片-五分钟学会Marp（上篇）, 连享会 No.656.\n宋森安, 2021, 用Markdown制作幻灯片-五分钟学会Marp（下篇）, 连享会 No.657.\n连玉君, 2022, Marp幻灯片模板：用Markdown快速写幻灯片, 连享会 No.1059.",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "body/01_3_markdown.html#用-markdown-做简历",
    "href": "body/01_3_markdown.html#用-markdown-做简历",
    "title": "6  Markdown",
    "section": "6.5 用 Markdown 做简历",
    "text": "6.5 用 Markdown 做简历\nMarkdown 也可以用来制作简历。参见：\n\nlapis-cv.bingyan.net，github\nresume.todev.cc/，github",
    "crumbs": [
      "**简介**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "body/05_write_with_AI.html",
    "href": "body/05_write_with_AI.html",
    "title": "7  使用 AI 撰写论文推介",
    "section": "",
    "text": "7.1 撰写提示词的建议\n对于初学者而言，精读 Top Journal 的论文，并参考作者提供的数据和代码进行复现是一个非常有效的学习方法。在此过程中，我们可以使用 VScode + Jupyter Notebook 来记录复现过程。虽然 AI 可以大幅提高复现和写作效率，但需要调教有方 (给出清晰的提示词)。\nA. 先泛聊。先使用比较开放的提示词，看看 AI 能否抓住这篇论文的要点，比如，请简要介绍论文的核心观点；论文的主要贡献是什么；简要说明文中理论模型的设定思路；该文的适用场景是什么？等；。也可以针对论文中比较棘手的部分进行提问，比如假设的含义、数学公式的解释等。\nB. 再形成提纲。待上述讨论完成后，先不要急着输出推文内容，而是让 AI 工具先给出一个提纲。然后根据提纲讨论完善后，再进行详细撰写。\nC. 分 Section 输出。对于 2000 字以上的内容，建议分多次输出。每次输出一个 Section，酌情修改和提出新的要求。符合要求后，再让 AI 按照这个风格继续输出后续内容。\n输出文档的提示词有如下几个要点：\n提示词，说明你的写作目的、读者群体和格式要求等",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>使用 AI 撰写论文推介</span>"
    ]
  },
  {
    "objectID": "body/05_write_with_AI.html#撰写提示词的建议",
    "href": "body/05_write_with_AI.html#撰写提示词的建议",
    "title": "7  使用 AI 撰写论文推介",
    "section": "",
    "text": "推文的目的，比如帮助读者快速了解论文的核心内容，或者提供对某个研究问题的深入分析；\n读者群体，比如经济学、计量经济学领域的博士生和青年教师/高年级本科生；\n风格要求：比如中文推文风格 (参考 https://www.lianxh.cn 的风格)；中文讲义风格 (语言朴实、用语准确) 等。\n优化提纲。可以用 再来一个版本；或 再来一个着重介绍 {论文思路/实证方法/因果识别策略} 的版本 等提示词，来让 AI 工具生成不同风格的内容。你也可以就某个 Section 提出具体要求，细化这个 Section 的提纲。\n\n\n\nTip 1：正式输出前，可以使用提示词 Prompt: 请为每个输出添加 label，方便我后续整理。这样 AI 工具会在每个 Section 前添加一个 label，比如 #Sec-1，#Sec-2 等，方便后续修改时引用。\nTip 2：可以使用提示词 Prompt: 请酌情标注需要插入原文图表的地方，并给出图形建议。随后，你可以使用 SniPaste 等软件从原文中截图后插入推文。\nTip 3：可以要求 AI 使用 $$ 符号包裹数学公式，方便后续在 Markdown 中渲染；并使用提示词 请勿添加任何表情符号，避免 AI 在输出中添加表情符号。\n\n\n\n写作目的：说明你希望通过这篇论文推介达到什么目的，比如帮助读者快速了解论文的核心内容，或者提供对某个研究问题的深入分析。\n读者群体：明确你的目标读者是谁，比如经济学、计量经济学领域的研究人员和学生。\n格式要求：说明你希望输出的文档格式，比如需要包含标题、摘要、引言、方法、结果、讨论等部分。\n内容要求：列出你希望 AI 工具覆盖的主要内容，比如论文的主要内容、研究方法、数据来源和主要结论等。\n篇幅要求：如果有特定的篇幅要求，可以在提示词中说明，比如希望生成一篇 5000 字以上的论文推介。你甚至可以要求每个 Section 的字数范围。如果推文中包含图表、公式或代码，可以限定中文字符的字数范围。\n分多次输出。对于 2000 字以上的内容，建议分多次输出。每次输出一个 Section，酌情修改和提出新的要求。符合要求后，再让 AI 按照这个风格继续输出后续内容。\n\n\n\n\n\n\n\n\n提示词：论文推介\n\n\n\n我想写一篇论文推介，介绍我传给你的 PDF 论文。具体要求如下：\n\n目标读者：经济学、计量经济学领域的研究人员和学生。\n\n希望能够让他们快速了解这篇论文的核心内容和贡献。\n\n对于一些复杂的数学公式，可以用比较通俗的语言进行解释。\n必要时，可以提供 1-2 个简化版的例子加以说明。\n\n需要介绍论文的主要内容、研究方法、数据来源和主要结论。\n如果需要插入原文中的图形或表格，请告知我图表编号和插入位置，我会手动添加。\n需要包含关键公式和代码的解读。\n需要提供参考文献列表。\n\n你先帮我列一个提纲，我们讨论完善后再进行详细撰写。\n\n以 Section 为单位输出。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>使用 AI 撰写论文推介</span>"
    ]
  },
  {
    "objectID": "body/05_write_with_AI.html#实例-1wing-et-al.-2024-论文推介",
    "href": "body/05_write_with_AI.html#实例-1wing-et-al.-2024-论文推介",
    "title": "7  使用 AI 撰写论文推介",
    "section": "7.2 实例 1：Wing et al. (2024) 论文推介",
    "text": "7.2 实例 1：Wing et al. (2024) 论文推介\nWing, C., Yozwiak, M., Hollingsworth, A., Freedman, S., & Simon, K. (2024). Designing Difference-in-Difference Studies with Staggered Treatment Adoption: Key Concepts and Practical Guidelines. Annual Review of Public Health, 45(1), 485–505. Link, PDF, Google, github-replication\n这篇论文介绍了五种常用的渐进 DID 的估计方法，作者还在 GitHub 上提供了相关的代码和数据。我采用上文介绍的提示词方法与豆包聊了 20 分钟，最终形成了推文的初稿 (满意度：80%)。\n\n豆包对话过程)\n\n\n\n\n\n\n\n提示词 1：概要\n\n\n\n8 论文解读：\n\nWing, C., Yozwiak, M., Hollingsworth, A., Freedman, S., & Simon, K. (2024). Designing Difference-in-Difference Studies with Staggered Treatment Adoption: Key Concepts and Practical Guidelines. Annual Review of Public Health, 45(1), 485–505. Link, PDF, Google.\n\n我想写一篇论文推介，介绍这篇论文。你帮我列个提纲。\n包括如下要点：\n\n论文的核心观点和主要结论是什么？\n文中介绍了哪些主要方法 (参见 Table 1)？\n作者提供了哪些实操建议？参见 Section 5",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>使用 AI 撰写论文推介</span>"
    ]
  },
  {
    "objectID": "body/05_write_with_AI.html#实例-2futurehouse.org-平台简介",
    "href": "body/05_write_with_AI.html#实例-2futurehouse.org-平台简介",
    "title": "7  使用 AI 撰写论文推介",
    "section": "8.1 实例 2：FutureHouse.org 平台简介",
    "text": "8.1 实例 2：FutureHouse.org 平台简介\n\nChatGPT 对话过程\n\nFutureHouse.org 平台提供了很多 AI 工具和资源，帮助用户更高效地使用 AI 技术。我浏览了该平台的主页和 Github 仓库后，将我记录的笔记提供给了 ChatGPT，并与它进行了一些讨论。最终，我让它帮我输出了一篇 5000 字的推文。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>使用 AI 撰写论文推介</span>"
    ]
  },
  {
    "objectID": "body/05_write_with_AI.html#实例-3概率匹配与-kelly-策略-python-代码",
    "href": "body/05_write_with_AI.html#实例-3概率匹配与-kelly-策略-python-代码",
    "title": "7  使用 AI 撰写论文推介",
    "section": "8.2 实例 3：概率匹配与 Kelly 策略-Python 代码",
    "text": "8.2 实例 3：概率匹配与 Kelly 策略-Python 代码\nChatGPT 对话过程\n我看到了一篇很有意思的 Blog：Probability matching and Kelly betting。作者介绍了概率匹配和 Kelly 策略的故事，提醒我们现实决策和自然演化中的“理性”，远不是单一的最大化期望值。更广义的理性关注风险、分布、长期增长以及群体的生存。\n里面有很多我不太理解的新概念，作者并未给出详细的解释。同时，我还想了解如何用 Python 代码实现这些概念。于是我将这篇 Blog 的内容和我的问题提供给了 ChatGPT，并与它进行了讨论。最终，我让它帮我输出了一篇 5000 字的推文 (初稿)。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>使用 AI 撰写论文推介</span>"
    ]
  },
  {
    "objectID": "body/05_write_with_AI.html#其它借助-ai-完成的推文",
    "href": "body/05_write_with_AI.html#其它借助-ai-完成的推文",
    "title": "7  使用 AI 撰写论文推介",
    "section": "8.3 其它借助 AI 完成的推文",
    "text": "8.3 其它借助 AI 完成的推文\n\n连小白, 2025, 自动化因果推断助手：Causal-Copilot 简介, 连享会 No.1643. (ChatGPT 对话)",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>使用 AI 撰写论文推介</span>"
    ]
  },
  {
    "objectID": "body/04_use_AI.html",
    "href": "body/04_use_AI.html",
    "title": "8  使用 AI 写论文推介",
    "section": "",
    "text": "8.1 局部翻译、润色\n写推文过程中，可能需要将英文内容翻译成中文，或对已有的中文内容进行润色。以下是一些常用的 AI 工具和提示词，帮助你完成翻译和润色工作。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>使用 AI 写论文推介</span>"
    ]
  },
  {
    "objectID": "body/04_use_AI.html#局部翻译润色",
    "href": "body/04_use_AI.html#局部翻译润色",
    "title": "8  使用 AI 写论文推介",
    "section": "8.2 数学公式识别",
    "text": "提示词-简要版\n\n\n\n请将以下内容翻译成中文，并进行润色：\n{{content}}\n\n\n\n\n\n\n\n\n提示词-升级版\n\n\n\n请将以下内容翻译成中文，并进行润色。 注意保留原文中的代码块和链接格式。 翻译时要确保语句通顺，符合中文表达习惯。 必要时可以意译，甚至大幅调整语序。\n{{content}}\n\n\n\n\n\n\n\n\n提示词-中文修改\n\n\n\n请修改、润色以下中文内容，确保语句通顺，符合中文表达习惯。 必要时可以意译、增加上下文或转承语句，甚至大幅调整语序。\n语言风格：兼顾上下文的同时，使用 lianxh.cn 推文中的语言风格。\n{{content}}\n\n8.2 数学公式识别\n识别数学公式的首选工具是 MathPix。\n替代工具：simpletex - 支持 在线识别；本地运行\n如果你没有 MathPix 账号，或不想使用付费版本，可以使用 DeepSeek 或 ChatGPT 等 AI 工具进行数学公式识别：\n\n截图：将需要识别的数学公式或文本截图保存为图片文件。建议使用 Snipaste 等工具进行截图。\n发送图片：将截图粘贴到 AI 工具的输入框中。\n输入提示词：根据所使用的 AI 工具，输入相应的提示词。\n\n\n\n\n\n\n\n提示词\n\n\n\n识别图片中的文字，转换成标准的 Markdown 文字。\n\n数学符号和公式使用 $$ 包围；\n- 文内公式格式为：$y = f(x)$\n- 单行公式格式为：\n\n$$\ny = f(x)\n$$\n\n随后我会发送一系列待识别的图片给你，请都按此要求识别。\n\n\n详情参见：连小白, 2025, 完美替代 MathPix：我用 ChatGPT、豆包、Kimi 识别数学公式.",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>使用 AI 写论文推介</span>"
    ]
  },
  {
    "objectID": "body/04_use_AI.html#数学公式识别",
    "href": "body/04_use_AI.html#数学公式识别",
    "title": "8  使用 AI 写论文推介",
    "section": "",
    "text": "识别数学公式的首选工具是 MathPix。\n替代工具：simpletex - 支持 在线识别；本地运行\n如果你没有 MathPix 账号，或不想使用付费版本，可以使用 DeepSeek 或 ChatGPT 等 AI 工具进行数学公式识别：\n\n截图：将需要识别的数学公式或文本截图保存为图片文件。建议使用 Snipaste 等工具进行截图。\n发送图片：将截图粘贴到 AI 工具的输入框中。\n输入提示词：根据所使用的 AI 工具，输入相应的提示词。\n\n\n\n\n\n\n\n提示词\n\n\n\n识别图片中的文字，转换成标准的 Markdown 文字。\n\n数学符号和公式使用 $$ 包围；\n- 文内公式格式为：$y = f(x)$\n- 单行公式格式为：\n\n$$\ny = f(x)\n$$\n\n随后我会发送一系列待识别的图片给你，请都按此要求识别。\n\n\n详情参见：连小白, 2025, 完美替代 MathPix：我用 ChatGPT、豆包、Kimi 识别数学公式.",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>使用 AI 写论文推介</span>"
    ]
  },
  {
    "objectID": "examples/AI_ideas_brainstorming.html",
    "href": "examples/AI_ideas_brainstorming.html",
    "title": "9  利用 AI 生成研究假设",
    "section": "",
    "text": "我主要受到了 Korinek (2023) 的启发。Korinek 教授为这篇文章建立了一个专门的网站：\n\n\n吴欣洋, 2025, AI自动生成研究假设，靠谱吗？流程与挑战",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>利用 AI 生成研究假设</span>"
    ]
  },
  {
    "objectID": "examples/AI_research_design_Erdem_2025.html",
    "href": "examples/AI_research_design_Erdem_2025.html",
    "title": "10  借助 AI 做研究设计",
    "section": "",
    "text": "10.1 下载文献\n这里以一篇 2025 年 7 月 15 日发布的工作论文为例，介绍如何使用 ChatGPT 解读这篇论文的要点，并让它给出使用该文方法研究中国问题的研究设计。\n在制作 Slides 的过程中，我需要搜索很多文献。由于时间和能力的限制，我无法对每篇文献进行深入的解读和总结。为此，我使用如下流程加速我对这些文献的理解：",
    "crumbs": [
      "**研究设计**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>借助 AI 做研究设计</span>"
    ]
  },
  {
    "objectID": "examples/AI_research_design_Erdem_2025.html#下载文献",
    "href": "examples/AI_research_design_Erdem_2025.html#下载文献",
    "title": "10  借助 AI 做研究设计",
    "section": "",
    "text": "使用我编写的 Stata 命令 getiref 获得引文信息，并下载 PDF 文档到指定路径。\n将这些 PDF 文档 (一次一篇或同主题的多篇) 发送给 AI 助手 (我主要使用 ChatGPT 和豆包)，让它们帮我解读论文的要点。我会酌情追问，并阅读原文中的相关内容。如有必要，我会把这些内容发送给 AI 助手，让它解读，或提供更容易理解的公式、实例、Stata/Python 代码等。\n\n\n10.1.1 用 getiref 下载引文\n你可以直接在 Stata 命令窗口中执行这些命令，也可以在 dofile 中执行。此处，我已经配置好了 Python + Jupyter Notebook 环境 (配置方法)，因此，我直接在 Jupyter Notebook 中执行这些命令。\n\n\n* 安装 getiref\nssc install getiref, replace\n\n* 设置下载路径\nglobal refpath \"D:\\github_lianxh\\AI_research\\examples\"\ncd \"$refpath\"\n\nUnknown #command\nUnknown #command\nchecking getiref consistency and verifying not already installed...\nall files already exist and are up to date.\nD:\\github_lianxh\\AI_research\\slides\\refs\n\n\n\ngetiref 2507.10933 , m arxiv pdf\n\n  \nErdem, O., & Ashok, R. P. (2025). Artificial Finance: How AI Thinks About Money\n&gt;  (Version 1). arXiv.\n    Link (rep)    PDF    Google    \n\nErdem, O., & Ashok, R. P. (2025). Artificial Finance: How AI Thinks About Money\n&gt;  (Version 1). arXiv. [Link](https://doi.org/10.48550/arXiv.2507.10933) (rep),\n&gt;  [PDF](https://arxiv.org/pdf/2507.10933.pdf), [Google](&lt;https://scholar.googl\n&gt; e.com/scholar?q=Artificial Finance: How AI Thinks About Money (Version 1)&gt;).\n        PDF:   dir    view_online     Open\n \nTips: Text is on clipboard. Press 'Ctrl+V' to paste, ^-^\n\n\n执行完上述命令后，引文信息已经被自动粘贴在剪切板上，直接按快捷键 Ctrl + V 即可粘贴到 Jupyter Notebook 中。同时，PDF 文档已经下载到指定路径。\n得到引文信息如下：\n\nErdem, O., & Ashok, R. P. (2025). Artificial Finance: How AI Thinks About Money (Version 1). arXiv. Link (rep), PDF, Google.",
    "crumbs": [
      "**研究设计**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>借助 AI 做研究设计</span>"
    ]
  },
  {
    "objectID": "examples/AI_research_design_Erdem_2025.html#论文解读",
    "href": "examples/AI_research_design_Erdem_2025.html#论文解读",
    "title": "10  借助 AI 做研究设计",
    "section": "10.2 论文解读",
    "text": "10.2 论文解读\n\n10.2.1 论文概要\n下面，我们开始写提示词，让 ChatGPT 帮忙解读这篇论文。你可以把该文的 PDF 文档发送给 ChatGPT (推荐，该方法比较稳定)，或直接在提示词中提供该文的链接。\n\n\n\n\n\n\n提示词 1：论文概要\n\n\n\n\nChatGPT 完整对话\n\n#任务：论文解读-Erdem, O., & Ashok, R. P. (2025)\n\nErdem, O., & Ashok, R. P. (2025). Artificial Finance: How AI Thinks About Money (Version 1). arXiv. [Link](https://doi.org/10.48550/arXiv.2507.10933) (rep), [PDF](https://arxiv.org/pdf/2507.10933.pdf), [Google](&lt;https://scholar.google.com/scholar?q=Artificial Finance: How AI Thinks About Money (Version 1)&gt;).\n\n要点：\n1. 主要研究内容\n2. 研究方法。与传统方法有何区别？\n3. 文中使用了那些主要的 AI 工具或模型？如何使用的？\n4. 作者提供的提示词有哪些？\n5. 这篇论文对我们进行金融学领域的研究有哪些启发？\n6. 酌情提供你认为重要的其他内容。 \n\n\nAI 基本上能提炼出全文的要点，与我快速浏览该文看到的信息基本一致。\n\n\n10.2.2 迁移：中国版研究设计\n\n\n\n\n\n\n提示词 2：迁移\n\n\n\n\nChatGPT 完整对话\n\n如果我想按照相似的模式和思路撰写一篇论文，我有如下问题：\n\n有哪些可能的选题 (列出 3 个)？每个选题的边际贡献和预期结论是什么？\n需要哪些大模型技术支持？预估费用和主要困难是什么？大概多久能完成？\n研究对象和数据来源：中国\n\n\n\nChatGPT 提供了三个选题建议，并针对我的问题 2 和 3 给出了详细的解答。三个选题如下：\n\nAI 如何理解中国家庭的储蓄与消费决策？\n大模型能否模拟中国人对风险的态度？——以保险选择为例\nAI 理解中国年轻人的职业选择逻辑吗？——一项基于 LLM 的偏好映射研究\n\n我最终选择第一个选题进行追问，让 ChatGPT 帮我设计了研究框架和方法。\n\n\n\n\n\n\n提示词 3：帮我做研究计划\n\n\n\n\nChatGPT 完整对话\n\n好，我计划研究第一个主题：AI 如何理解中国家庭的储蓄与消费决策？\n你帮我制定一个具有可操作性的研究计划\n\n\nAI 做的研究计划看起来流程上没有问题，但细节上有很多问题。比如，没有明确数据来源于何处，也没有说明关键方法的细节。此时，我们需要快速阅读原文，找出研究设计中最重要 (通常也最棘手) 的一些问题进行追问。这是与 AI 进行合作的关键所在。\n\n\n\n10.2.3 追问、细化研究设计\n\n\n\n\n\n\n提示词 4：追问、细化\n\n\n\n在 Erdem et al. (2025) 的论文中，人类数据源于一个非常成熟的数据库 「人类数据：来自 INTRA（International Test on Risk Attitudes）数据库（Wang et al., 2017）」。 我的问题： 1. 我的研究中使用哪个数据源比较好？ 2. Erdem 文中设计了 14 个问题，我的研究中如何设计这些问题？能否给我一个范本？\n\n\n这次 AI 的表现不错，帮我设计了 14 个问题，涵盖了储蓄、消费、风险偏好等方面。并且给出了每个问题的设计思路和预期目标。\n它很聪明，主动提出了如下建议 (这个是我此前没有想到的)：\n\n\n\n\n\n\nChatGPT 的建议 1\n\n\n\n如你需要，我可以：\n直接帮你整理出 14 个问题的题库文案；\n为每个问题生成 GPT-4o、Gemini、通义等模型的标准化 prompt；\n输出配套 Python 提示词实验脚本框架。\n是否要我继续整理出完整的「14 个问题题库」？\n\n\n\n\n\n\n\n\n提示词 5：让 AI 写 Prompt 脚本和 Python 代码\n\n\n\n\nChatGPT 完整对话\n\n好，请完成如下任务：\n直接帮你整理出 14 个问题的题库文案；\n为每个问题生成 GPT-4o、Gemini、通义等模型的标准化 prompt；\n输出配套 Python 提示词实验脚本框架。\n\n\nChatGPT 帮我编写了一些 Prompts 和 Python 代码，虽然有些问题，但基本上可以直接使用。同时，他还给出了如下建议：\n\n\n\n\n\n\nChatGPT 的建议 2\n\n\n\n如果你使用国内模型（如通义），我可帮你将 API 接口改为 dashscope；\n如果你希望将输出结果直接转为结构化 JSON 或进一步分析（如聚类、PCA），我也可以补上后续代码模板；\n如果需要生成配套的问卷版 PDF 或问卷星模板，也可以告诉我，我可代为整理。\n是否继续？你下一步准备运行实验，还是要先发放人类问卷做对比？\n\n\n我对通义千问的 API 接口不熟悉，因此，我没有继续和 ChatGPT 对话，而是另开了一个对话窗口，询问通义千问的 API 接口使用方法。10 分钟后，我大致理解了 API 的原理：本质上就是按照一定的语法规则把提示词和参数传递给通义千问，获取返回结果 (有点类似于使用 APP 点菜)。\n我注意到，上述建议中其实包含了多个操作步骤，我想逐个处理。为此，我又编写了如下提示词：\n\n\n\n\n\n\n提示词 6：让 AI 使用通义千问编写代码\n\n\n\n\nChatGPT 完整对话\n\n我用通义千问，你帮我编写一份完整的代码思路文本 (伪代码)，然后撰写完整的可以实操的 Python 代码。\n这一步完成后，再开始制作基于问卷星的人类问卷。\n\n\nAI 帮我编写了一个 Python 脚本，脚本中包含了如何使用通义千问的 API 接口获取结果的代码。虽然有些问题，但基本上可以直接使用。\n\n\n10.2.4 棘手问题如何应对\n我翻看原文以及 ChatGPT 对论文的总结后，发现了一个有趣的问题：AI 在回答问题时，似乎更倾向于非西方国家的文化偏好。这似乎也不难理解，因为在最初训练阶段，人工标注者主要来自非西方国家 (如东非的肯尼亚、坦桑尼亚等)，因此模型的回答模式也更接近这些国家的文化偏好。\n然而，这个问题在我的研究设计中可能会带来一些挑战。为此，我又编写了如下提示词：\n\n\n\n\n\n\n提示词 7：棘手问题如何应对\n\n\n\n你提到：\n’’’\n启发 2：语言模型训练影响文化偏好 LLM 的整体回答模式最接近坦桑尼亚（Tanzania），而非传统的西方国家（WEIRD）。\n可能与模型训练中用于 RLHF（reinforcement learning from human feedback）的人工标注者地域有关，尤其是东非国家如肯尼亚、坦桑尼亚广泛参与了 GPT 系列模型的安全微调。\n’’’\n在我的研究设计中，这个问题怎么应对？\n\n\nChatGPT 非常认同我的担忧，并提出了 4 中应对方案。截取 ChatGPT 的回答如下：\n\n\n\n\n\n\nChatGPT 的回应\n\n\n\n\nChatGPT 完整对话\n\n这是一个非常关键的问题。你指出的是：LLM 回答中可能蕴含其训练过程中来自特定文化背景的偏见，而非真正理解“普遍的人类行为”或“中国家庭行为”。在你的研究「AI 如何理解中国家庭的储蓄与消费决策？」中，这种文化偏置会对结论构成干扰，甚至掩盖模型与中国家庭之间的真实差异。\n下面是针对这个问题的系统性应对方案 (详情参见 ChatGPT 完整对话)：\n\n策略一：纳入对照组与“文化距离”分析\n策略二：提示词中嵌入文化人格（persona）\n策略三：将文化偏置作为独立研究问题呈现\n策略四：将部分 prompt 翻译为英文再提交 LLM\n\n\n\n我逐渐意识到，刚才讨论的这个问题其实是一个非常棘手的问题。它有可能是这篇论文的硬伤。如果是的话，我就需要慎重考虑是否继续这个研究主题。\n为此，我又进行了追问，提示词如下：\n\n\n\n\n\n\n提示词 8：棘手问题的底层逻辑\n\n\n\n这里涉及到一个大模型的底层逻辑问题：\n通义千问的人工标签是谁做的？\n如果是中国人的话，是否分布在特定的省份或文化圈？\n如果通义千问是基于 Open AI 开发的，具有蒸馏特征，那我们针对中国的研究必然也存在于 Erdem 2025 相同的难题。\n你怎么看待这个问题？\n\n\nChatGPT 认为这是一个非常关键的问题，但它很能「掰扯」，回应如下：\n\n\n\n\n\n\nChatGPT 的回应\n\n\n\n三、这是否削弱你研究的可行性？我怎么看？ 我的观点是：\n这并不削弱你的研究价值，反而强化了它的必要性。\n若通义千问也表现出与中国家庭数据的“文化不一致”——说明“中文模型 ≠ 中国文化模型”\n你的研究将是验证国产大模型是否真正理解中国人经济行为的第一批系统性工作之一\n你可以将这一问题设计为研究目标本身的一部分，例如：\n“即使通义千问是中文预训练的大模型，其 RLHF 阶段是否真实反映了中国家庭行为偏好？我们在本研究中通过对比中国人类数据与模型输出，检验其行为合理性与文化一致性。”\n这也带来实践上的衍生价值：\n可为阿里、百度、商汤等模型开发者提供“行为偏好适配反馈”\n可为政策类 AI（如金融推荐、社保系统）提供风险警告：当前大模型输出可能并非“多数中国人”的最优选择\n\n\n我的直觉告诉我，虽然 ChatGPT 的说法有一定的道理，但想要把这个硬伤包装成论文的亮点并非易事。为此，我追问道：\n\n\n\n\n\n\n提示词 9：硬伤如何包装成亮点 ？\n\n\n\n我在研究设计中应该如何应对可能来自审稿人的质疑？\n如何把这个潜在的硬伤「包装」或「论述」为我这篇文章的亮点？\n有没有可能同时使用多个大模型开展这项研究，然后进行对比或进行 model averaging 这样的处理？\n\n\nChatGPT 的原始回答非常有意思，给出了比较具体的建议和应对方法。整体上，我觉得用它的办法，真的有可能把这个硬伤包装成亮点。\n下面是对 ChatGPT 的回答进行的简要总结：\n\n\n\n\n\n\nChatGPT 的回应\n\n\n\n\n如何应对审稿人对“大模型文化偏误”的质疑？\n\n可能质疑的角度：\n\n\n\n\n\n\n\n质疑类型\n审稿人可能说的话\n\n\n\n\n模型文化偏见\n“大模型的行为是否真的代表 AI？还是只是某些训练数据群体的平均值？”\n\n\n模型非确定性\n“LLM 的输出具有随机性，重复运行会变吗？”\n\n\n模型泛用性\n“为何只用通义千问？是否能推广到其它模型或语言环境？”\n\n\n\n \n\n模型文化偏误的挑战与研究意义\n\n尽管通义千问等中文大模型由中国团队开发，但其训练数据和 RLHF 标签仍可能集中于特定省份或文化圈，未必能真实反映全国性或多样化的家庭行为模式。因此，本研究不仅要检验模型是否能“说中文”，更要问：它是否“理解中国人”？\n\n从潜在缺陷转化为研究价值\n\n我们将模型输出与中国家庭调查数据系统对比，不将偏差视为缺陷，而是将其视为测量 AI 对本土文化理解能力的窗口。这为未来模型微调、金融推荐系统设计提供了文化适配性的评估工具。\n\n多模型对比策略\n\n为增强稳健性，我们引入 GPT-4o、通义千问、Moonshot 等多模型并行实验，构建“行为向量”进行主成分分析与行为聚类。模型之间的偏离，正是 AI 行为文化特征的重要体现，有助于识别模型是否具有结构性偏见。\n\n\n\n模型\n平台\n特点\n\n\n\n\nGPT-4o\nOpenAI\n英语训练偏重、蒸馏自西方行为逻辑\n\n\n通义千问\n阿里\n中文对话、训练数据偏官方\n\n\nMoonshot v1.5\nMiniMax\n泛中文市场偏创业用户群体\n\n\nClaude 3\nAnthropic\n对“社会公正”“中立”敏感度高\n\n\n\n \n\n对比这些模型在相同提示词、同一问题下的输出结构、逻辑倾向、风险偏好。",
    "crumbs": [
      "**研究设计**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>借助 AI 做研究设计</span>"
    ]
  },
  {
    "objectID": "examples/AI_research_design_Erdem_2025.html#总结",
    "href": "examples/AI_research_design_Erdem_2025.html#总结",
    "title": "10  借助 AI 做研究设计",
    "section": "10.3 总结",
    "text": "10.3 总结\n整体上，我对 ChatGPT 的表现还是比较满意的。它不仅能快速提炼出论文的要点，还能根据我的需求设计出可操作的研究计划，并提供了相应的代码框架。\n最主要的是，如果仅仅是看论文原文，我可能无法快速发现这篇论文的潜在硬伤。另外，如果要将这篇论文的方法迁移过来做中国问题，以我对大模型和 Python 的掌握程度，我基本上无法做出合理的研究设计。在与 ChatGPT 的对话中，我会学习它提到的新概念、新名词，通过干中学的方式不断完善我对这类问题的整体认知。\n我强烈建议各位读者找一篇最新的 working paper，使用上述方法进行研读，并尝试进行研究设计。即使你最终不选择做这个主题，但也能从中学到不少研究设计和计量方法的技巧。",
    "crumbs": [
      "**研究设计**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>借助 AI 做研究设计</span>"
    ]
  },
  {
    "objectID": "examples/AI_research_design_Erdem_2025.html#附llm_savings_prompts_qwen.py",
    "href": "examples/AI_research_design_Erdem_2025.html#附llm_savings_prompts_qwen.py",
    "title": "10  借助 AI 做研究设计",
    "section": "10.4 附：llm_savings_prompts_qwen.py",
    "text": "10.4 附：llm_savings_prompts_qwen.py\n\nChatGPT 完整对话\n\n这个 Python 脚本是我向 ChatGPT 提出「提示词 6：让 AI 使用通义千问编写代码」后，他自动编写的，包括如下四个部分：\n\n中文题库设计 (共 14 题)\n标准化 Prompt 模板（支持通义千问）\n伪代码：通义千问版本的实验流程\n通义千问实操代码（DashScope API）\n\nps，由于时间的关系，我还没有尝试运行这个脚本，有兴趣的读者可以测试，并将您的测试结果反馈给我。\n# 文件：llm_savings_prompts_qwen.py\n\n# -----------------------------\n# 1. 中文题库设计（共 14 题）\n# -----------------------------\n\nquestions = [\n    # 时间偏好\n    \"假设你现在可以获得 1,000 元，或者等待 1 年获得 X 元。请问，X 至少是多少，才能让你愿意等待一年？\",\n    \"假设你可以立刻获得 2,000 元，或者在 10 年后获得 Y 元。请问，Y 至少是多少，才能让你愿意等待？\",\n\n    # 收入分配\n    \"如果你的下月工资多出 2,000 元，你会如何分配这笔钱？请用百分比表示：食品 __%、娱乐 __%、教育 __%、储蓄 __%。\",\n    \"如果你的收入减少 1,000 元，你最可能减少哪一项支出？A. 食品 B. 教育 C. 娱乐 D. 储蓄。请简要说明理由。\",\n\n    # 预防性储蓄\n    \"假设你得知你父亲未来一年可能需要 10,000 元医疗支出，你是否会调整当前的消费？你愿意每月减少多少元用于储蓄？\",\n    \"你是否有每月固定的预防性储蓄计划（如应对失业、健康支出）？如有，请简要说明金额与动机。\",\n\n    # 风险认知与应对\n    \"假设你所在行业未来一年有 10% 的失业概率，你是否会因此增加储蓄？若会，预计增加多少元/月？\",\n    \"假设某保险产品年保费 800 元，可覆盖重大疾病 30 万元赔偿。你是否愿意购买？请说明理由。\",\n\n    # 家庭生命周期\n    \"你是否因为孩子的教育而主动减少自己的日常消费？请列举最近一次这样的决策和金额。\",\n    \"如果你刚生育一个孩子，你会如何调整以下支出比例（教育 __%、娱乐 __%、储蓄 __%、其他 __%）？\",\n\n    # 消费偏好\n    \"请列举你最近三个月最大的一笔消费支出，并说明其动因（如：必要/非必要、情绪驱动、家庭责任等）。\",\n    \"在没有收入压力的前提下，你更倾向于提前消费还是储蓄？请简要说明原因。\",\n\n    # 财富认知\n    \"你认为一个家庭是否有必要维持至少 6 个月的生活费作为紧急储备？为什么？\",\n    \"你认为影响储蓄倾向最重要的因素是什么？A. 收入 B. 教育水平 C. 风险意识 D. 家庭责任感。请说明理由。\"\n]\n\n# -----------------------------\n# 2. 标准化 Prompt 模板（支持通义千问）\n# -----------------------------\n\nprompt_template = \"\"\"\n以下是一个关于中国家庭储蓄与消费决策的情境问题。\n请根据你的判断，独立作答。\n\n问题：{question}\n\n请只输出你的答案，不需要解释或重复题干。\n如果涉及金额，请直接给出数字或百分比。\n\"\"\"\n\ndef build_prompts(questions):\n    prompts = []\n    for q in questions:\n        prompts.append(prompt_template.format(question=q))\n    return prompts\n\n\n# -----------------------------\n# 3. 伪代码：通义千问版本的实验流程\n# -----------------------------\n\n\"\"\"\n1. 读取题库，构造 prompts\n2. 设置通义 dashscope API 的参数（如模型 qwen-plus）\n3. 每题生成 n_repeat 次回答\n4. 所有回答结果保存为 CSV\n5. 可选：记录响应时间、异常处理信息\n\"\"\"\n\n# -----------------------------\n# 4. 通义千问实操代码（DashScope API）\n# -----------------------------\n\nimport dashscope\nimport time\nimport pandas as pd\n\n# 替换为你的通义 API Key\nDASHSCOPE_API_KEY = \"YOUR_DASHSCOPE_API_KEY\"\ndashscope.api_key = DASHSCOPE_API_KEY\n\n# 构建 prompts\nprompts = build_prompts(questions)\n\nmodel_name = \"qwen-plus\"\nn_repeat = 100\n\ndata = []\n\nfor qid, prompt in enumerate(prompts):\n    for trial in range(n_repeat):\n        try:\n            response = dashscope.Generation.call(\n                model=model_name,\n                prompt=prompt,\n                temperature=0.7,\n                top_p=0.9,\n                result_format=\"message\"\n            )\n            answer = response.output.choices[0].message.content.strip()\n            data.append({\n                \"question_id\": qid + 1,\n                \"trial\": trial + 1,\n                \"response\": answer\n            })\n            time.sleep(0.3)  # 限速保护\n        except Exception as e:\n            print(f\"错误：Q{qid+1}-T{trial+1}: {e}\")\n            time.sleep(2)\n\n# 保存为 CSV\npd.DataFrame(data).to_csv(\"qwen_savings_responses.csv\", index=False)\nprint(\"实验完成，结果已保存。\")",
    "crumbs": [
      "**研究设计**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>借助 AI 做研究设计</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html",
    "href": "examples/learn_Tobit_model.html",
    "title": "11  使用 AI 解读 Tobit 模型",
    "section": "",
    "text": "11.1 简介\n在学习计量经济学的过程中，许多学生常常会遇到复杂的模型和晦涩的数学推导，感到无从下手。由于缺乏合适的老师或同伴指导，理解这些内容变得更加困难。\nAI 助手的出现，为学生们提供了全新的学习方式。通过 AI，学生不仅可以获得模型原理、数学表述和经济学含义的详细讲解，还能根据自己的需求生成 Stata 或 Python 代码、图形和 LaTeX 公式。\n更重要的是，AI 能用讲义风格的文字和丰富的代码注释，帮助学生逐步掌握知识点。无论你的基础如何，AI 都可以用通俗易懂的语言，甚至模拟经验丰富的教授，帮助你找到最适合自己的学习路径。\n本节的目的是通过 AI 助手，帮助学生理解 Tobit 模型的原理、数学表述和经济学含义。我们将展示如何使用 AI 生成相关内容，并提供实际的代码示例和图形展示。\n我主要受到了 Korinek (2023) 的启发。为了解读 Solow 模型的原理 (Chat 50)，Korinek 向 AI 发送了提示词 Simulate the Solow growth model and plot the results in a four chart.。AI 的解读效果非常好。",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html#模拟-tobit-modelpython-代码",
    "href": "examples/learn_Tobit_model.html#模拟-tobit-modelpython-代码",
    "title": "11  使用 AI 解读 Tobit 模型",
    "section": "11.2 模拟 Tobit Model：Python 代码",
    "text": "11.2 模拟 Tobit Model：Python 代码\n\n\n\n\n\n\n提示词\n\n\n\n\n任务：写一段 {Python} 代码，模拟 Tobit Model 的数据，并酌情绘图。\n\nN = 500；\n支持中文字符显示\n屏蔽 warning 信息\n\n目的：帮助初学者理解 Tobit Model 的数据生成过程。\n风格：讲义风格，核心代码要添加中文注释\n\n\n\n\nimport warnings\nimport numpy as np\n\nwarnings.filterwarnings('ignore')  # 屏蔽所有 warning 信息\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体为黑体\nplt.rcParams['axes.unicode_minus'] = False    # 正常显示负号\n\n# 设置随机种子，保证结果可复现\nnp.random.seed(42)\n\n# 样本量\nN = 500\n\n# 生成自变量 X，服从标准正态分布\nX = np.random.normal(0, 1, N)\n\n# 设置 Tobit 模型的参数\nbeta_0 = 1.0   # 截距\nbeta_1 = 2.0   # 斜率\nsigma = 1.0    # 残差标准差\n\n# 生成潜在因变量 y*（未截断前的真实值）\ny_star = beta_0 + beta_1 * X + np.random.normal(0, sigma, N)\n\n# 设置截断点（左截断为0）\ny = np.maximum(y_star, 0)\n\n# 绘图：展示 Tobit 数据的分布\nplt.figure(figsize=(10, 5))\n\n# 散点图：X 与 y* 的关系\nplt.subplot(1, 2, 1)\nplt.scatter(X, y_star, alpha=0.5, label='y*（未截断）')\nplt.axhline(0, color='red', linestyle='--', label='截断点')\nplt.title('X 与 y*（未截断）')\nplt.xlabel('X')\nplt.ylabel('y*')\nplt.legend()\n\n# 散点图：X 与 y 的关系\nplt.subplot(1, 2, 2)\nplt.scatter(X, y, alpha=0.5, label='y（截断后）')\nplt.axhline(0, color='red', linestyle='--', label='截断点')\nplt.title('X 与 y（截断后）')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html#原理和模型表述",
    "href": "examples/learn_Tobit_model.html#原理和模型表述",
    "title": "11  使用 AI 解读 Tobit 模型",
    "section": "11.3 原理和模型表述",
    "text": "11.3 原理和模型表述\n\n\n\n\n\n\n提示词\n\n\n\n写一段讲义风格的文字，用 LaTeX 公式描述 Tobit Model 的原理和模型表述。\n然后结合上面的代码和图形解释公式的经济学含义，最好能结合一些容易理解的实例。\n\n\nTobit 模型是一种用于处理因变量存在截断（如左截断为 0）的回归模型。其基本思想是：我们观测到的因变量 \\(y\\)，是一个潜在的连续变量 \\(y^*\\) 被某个阈值截断后的结果。",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html#tobit-模型的数学表述",
    "href": "examples/learn_Tobit_model.html#tobit-模型的数学表述",
    "title": "11  使用 AI 解读 Tobit 模型",
    "section": "11.4 Tobit 模型的数学表述",
    "text": "11.4 Tobit 模型的数学表述\nTobit 模型可表示为：\n\\[\n\\begin{align*}\ny_i^* &= \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2) \\\\\ny_i &= \\begin{cases}\ny_i^*, & \\text{如果 } y_i^* &gt; 0 \\\\\n0,    & \\text{如果 } y_i^* \\leq 0\n\\end{cases}\n\\end{align*}\n\\]\n其中： - \\(y_i^*\\)：不可观测的潜在因变量（如“理想消费”） - \\(y_i\\)：实际观测到的因变量（如“实际消费”，不能为负） - \\(x_i\\)：自变量 - \\(\\beta_0, \\beta_1\\)：回归系数 - \\(\\varepsilon_i\\)：正态分布的误差项",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html#经济学含义与实例",
    "href": "examples/learn_Tobit_model.html#经济学含义与实例",
    "title": "11  使用 AI 解读 Tobit 模型",
    "section": "11.5 经济学含义与实例",
    "text": "11.5 经济学含义与实例\n以家庭消费为例，\\(y_i^*\\) 可以理解为家庭的“理想消费金额”，但由于某些家庭可能没有消费（如收入太低），实际观测到的消费 \\(y_i\\) 就会被截断为 0。\n在上面的代码中，我们模拟了 \\(N=500\\) 个样本，生成了 \\(y^*\\) 和 \\(y\\)。图形左侧展示了 \\(X\\) 与 \\(y^*\\) 的关系，可以看到 \\(y^*\\) 是连续分布的；右侧展示了 \\(X\\) 与 \\(y\\) 的关系，所有 \\(y^* \\leq 0\\) 的点都被截断为 0，形成了“堆积”。\n这种模型广泛用于： - 消费/投资为零的情况（如部分家庭没有购房支出） - 信贷申请/批准金额（如部分人未获批贷款） - 劳动供给（如部分人未参与劳动市场）\nTobit 模型能有效处理这类“部分观测为零”的数据，避免 OLS 回归的偏误。",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html#详细推导过程",
    "href": "examples/learn_Tobit_model.html#详细推导过程",
    "title": "11  使用 AI 解读 Tobit 模型",
    "section": "11.6 详细推导过程",
    "text": "11.6 详细推导过程\n其实，仅仅依据上面的内容，你还是无法真正理解 Tobit 模型的数学公式和经济学含义。下面我们将详细推导 Tobit 模型的数学公式，并解释每一步的经济学含义。\n我列举了一些提示词要点，你可以酌情选择使用。建议一次性不要给 AI 太多提示词，分步进行更容易获得满意的结果。\n\n\n\n\n\n\n提示词\n\n\n\n写一段讲义风格的文字，详细推导 Tobit 模型的数学公式。包括：\n\n潜在变量 \\(y^*\\) 的定义\n截断后的观测变量 \\(y\\) 的定义\n期望值和条件期望值的推导\nTobit 和 OLS 估计式的关系\nTobit 模型的最大似然估计方法\nTobit 模型边际效应的计算方法和经济含义\n\n\n\n\n11.6.1 Tobit 模型的详细数学推导\n\n11.6.1.1 1. 潜在变量 \\(y^*\\) 的定义\nTobit 模型假设存在一个不可观测的潜在连续变量 \\(y^*\\)，其生成机制为： \\[\ny_i^* = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2)\n\\] 其中，\\(x_i\\) 是自变量，\\(\\beta_0, \\beta_1\\) 为参数，\\(\\varepsilon_i\\) 为正态分布误差项。\n\n\n11.6.1.2 2. 截断后的观测变量 \\(y\\) 的定义\n实际观测到的因变量 \\(y_i\\) 受到截断（如左截断为 0）： \\[\ny_i = \\begin{cases}\ny_i^*, & \\text{如果 } y_i^* &gt; 0 \\\\\n0, & \\text{如果 } y_i^* \\leq 0\n\\end{cases}\n\\]\n\n\n11.6.1.3 3. 期望值和条件期望值的推导\nTobit 模型下，\\(y_i\\) 的条件期望为： \\[\nE[y_i|x_i] = P(y_i^* &gt; 0|x_i) \\cdot E[y_i^*|y_i^* &gt; 0, x_i]\n\\] 具体推导如下：\n\n\\(P(y_i^* &gt; 0|x_i) = 1 - \\Phi\\left(-\\frac{\\beta_0 + \\beta_1 x_i}{\\sigma}\\right)\\)，其中 \\(\\Phi(\\cdot)\\) 为标准正态分布函数。\n\\(E[y_i^*|y_i^* &gt; 0, x_i] = \\beta_0 + \\beta_1 x_i + \\sigma \\frac{\\varphi\\left(\\frac{\\beta_0 + \\beta_1 x_i}{\\sigma}\\right)}{1 - \\Phi\\left(-\\frac{\\beta_0 + \\beta_1 x_i}{\\sigma}\\right)}\\)，其中 \\(\\varphi(\\cdot)\\) 为标准正态密度函数。\n\n因此， \\[\nE[y_i|x_i] = \\left[1 - \\Phi\\left(-\\frac{\\beta_0 + \\beta_1 x_i}{\\sigma}\\right)\\right] \\left[\\beta_0 + \\beta_1 x_i + \\sigma \\frac{\\varphi\\left(\\frac{\\beta_0 + \\beta_1 x_i}{\\sigma}\\right)}{1 - \\Phi\\left(-\\frac{\\beta_0 + \\beta_1 x_i}{\\sigma}\\right)}\\right]\n\\]\n\n\n11.6.1.4 4. Tobit 和 OLS 估计式的关系\nOLS 直接对 \\(y\\) 回归会低估 \\(\\beta_1\\)，因为截断导致大量 \\(y=0\\) 的观测值，拉低了回归斜率。Tobit 模型通过最大似然法，利用了 \\(y=0\\) 的概率信息，能更准确估计参数。\n\n\n11.6.1.5 5. Tobit 模型的最大似然估计方法\nTobit 模型的对数似然函数为： \\[\n\\ell(\\beta_0, \\beta_1, \\sigma) = \\sum_{y_i=0} \\log \\Phi\\left(\\frac{-(\\beta_0 + \\beta_1 x_i)}{\\sigma}\\right) + \\sum_{y_i&gt;0} \\log \\left[\\frac{1}{\\sigma} \\varphi\\left(\\frac{y_i - \\beta_0 - \\beta_1 x_i}{\\sigma}\\right)\\right]\n\\] 其中，第一项对应被截断的观测，第二项对应未被截断的观测。通过极大化该似然函数，得到参数估计值。\n\n\n11.6.1.6 6. Tobit 模型边际效应的计算方法和经济含义\nTobit 模型的系数 \\(\\beta_1\\) 并不是 \\(x\\) 对 \\(y\\) 的直接边际效应。\\(x\\) 对 \\(E[y|x]\\) 的边际效应为： \\[\n\\frac{\\partial E[y|x]}{\\partial x} = \\beta_1 \\cdot \\Phi\\left(\\frac{\\beta_0 + \\beta_1 x}{\\sigma}\\right)\n\\] 这表示 \\(x\\) 的影响被截断概率所“缩小”，只有未被截断的部分才体现出 \\(x\\) 的真实效应。经济学上，这反映了自变量对“实际观测到的因变量”的平均影响。\n\n通过上述推导，可以系统理解 Tobit 模型的结构、推断方法及其与 OLS 的区别，并掌握边际效应的实际含义。",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html#tobit-和-ols-对比",
    "href": "examples/learn_Tobit_model.html#tobit-和-ols-对比",
    "title": "11  使用 AI 解读 Tobit 模型",
    "section": "11.7 Tobit 和 OLS 对比",
    "text": "11.7 Tobit 和 OLS 对比\n你还可以进步借助 AI 生成 Tobit 和 OLS 模型的估计结果图示，以便说明 OLS 估计可能存在的偏差。\n这里用了教育学中一个非常重要的原理：知识迁移。在学习 Tobit 模型过程中，你可以借助对比，将已经掌握的 OLS 模型知识迁移到新的 Tobit 模型上。\n\n\n\n\n\n\n提示词\n\n\n\n\n目的：编写 Python 代码，图示 Tobit 和 OLS 模型的估计结果，以便说明 OLS 估计可能存在的偏差\n风格：讲义风格，核心代码要添加中文注释\n代码要求：\n\n要能独立运行，不依赖与此前代码块的设定\n支持中文字符显示；屏蔽 warning 信息",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html#后续讨论",
    "href": "examples/learn_Tobit_model.html#后续讨论",
    "title": "11  使用 AI 解读 Tobit 模型",
    "section": "11.8 后续讨论",
    "text": "11.8 后续讨论\n按照上述逻辑，你可以不断追问 AI，生成更多内容。这种学习模式与传统的课堂教学和硬啃教科书有很大区别。你完全可以根据自己的基础和需求，调整 AI 的提示词，让其生成最适合你的内容。\n比如，你可以使用如下提示词追问：\n\n\n\n\n\n\n提示词\n\n\n\n\nTobit 模型如何估计？\n如何解释 Tobit 模型的系数含义？边际效应如何计算？\n有哪些 Tobit 模型的拓展模型？\n在因果推断中，有哪些与 Tobit 模型或者其模型设定思想相关的模型？它们的适用场景是什么？\n对于面板数据而言，有哪些主要的 Tobit 模型？\n帮我推荐 3-5 篇公司金融领域使用 Tobit 或相关模型进行实证分析的论文。要求：近 10 年发表的；仅限于 JFE, JF, RFS, JFQA, JCF, RFS 等顶级期刊；最好是有数据和代码的。\n\n\n\n\nChatGPT 对话过程",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html#参考文献",
    "href": "examples/learn_Tobit_model.html#参考文献",
    "title": "11  使用 AI 解读 Tobit 模型",
    "section": "11.9 参考文献",
    "text": "11.9 参考文献\n\nKorinek, A. (2023). Generative AI for Economic Research: Use Cases and Implications for Economists. Journal of Economic Literature, 61(4), 1281–1317. Link (rep), PDF, Appendix, Google.\n\n更新 1：2024 版全文更新\n更新 2：提供了很多 Prompts",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/00_intro.html",
    "href": "examples/CSMAR/00_intro.html",
    "title": "12  数据分析案例：CAMAR 数据库合并",
    "section": "",
    "text": "12.1 多个 zip 文件的合并\n这个 Jupyter Notebook 文件演示了如何将多个 zip 文件中的 CSV 数据合并成一个大的数据集。我们将使用 Python 的 pandas 库来处理数据。在进行这个任务是，我刚接触 Python 一周，只了解一些基本概念，没有任何实操经验。如果不借助 AI，我甚至难以写出两行正确的代码。\n不过，好在有 Copilot 的帮助，我只需要写清楚提示词，它基本上能够一次性帮我编写好代码。期间很少出现 bugs。当然，很多时候由于我对数据特征了解不够，往往无法准确表述我的需求，这时我会通过多次迭代来完善提示词，直到得到满意的结果。\n需要注意的是，在这个过程中，我很少会动手修改 Copilot 生成的代码。我的主要精力集中在修改和优化提示词上，我会不断把更新后的提示词发给 Copilot，让它生成新的代码。运行后，如果不符合要求，或我有新的数据处理需求，我会再次调整提示词，直到得到满意的结果。",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>数据分析案例：CAMAR 数据库合并</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/00_intro.html#多个-zip-文件的合并",
    "href": "examples/CSMAR/00_intro.html#多个-zip-文件的合并",
    "title": "12  数据分析案例：CAMAR 数据库合并",
    "section": "",
    "text": "01_unzip_to_csv 文件\n\n\n\n\n\n12.1.1 任务概况\n原始数据文件是从 CSMAR 数据库下载的，包含多个 zip 文件：\n\n我们需要将这些 zip 文件自动解压成 CSV 文件，并合并成一个大的数据集。",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>数据分析案例：CAMAR 数据库合并</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html",
    "href": "examples/CSMAR/01_unzip_to_csv.html",
    "title": "13  CSMAR 数据处理",
    "section": "",
    "text": "13.1 从 CAMAR 数据库下载 Excel 数据文件\nCSMAR (China Stock Market & Accounting Research) 是一个提供中国上市公司数据的数据库。本文将介绍如何使用 CSMAR 数据库获取上市公司基本信息和财务资料，并经过合并、清洗等处理，得到清洁数据。",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#从-camar-数据库下载-excel-数据文件",
    "href": "examples/CSMAR/01_unzip_to_csv.html#从-camar-数据库下载-excel-数据文件",
    "title": "13  CSMAR 数据处理",
    "section": "",
    "text": ".csv 优于 .xlsx\n\n\n\n我此前对 Python 处理数据的机制了解不足，导致我最初下载的都是 .xlsx 格式的数据。虽然这种 Excel 格式的数据可以使用 pandas 读取，但非常耗时 (一份包含 8w 行观察值的资产负债表大概需要 50s)。相比之下，.csv 格式的数据读取速度更快，通常只需要几秒钟。\n因此，大家从 CSMAR 数据库下载数据时，建议选择 .csv 格式，而不是 .xlsx 格式。\n\n\n\n13.1.1 数据下载页面\n\n网址：https://data.csmar.com/\n登录：中大 IP 地址范围内自动登录 (机构账号)\n\n\n主要有两种查询方案：\n\n单表查询：直接查询某一张表格的数据。\n多表查询：可以跨表查询，形成一个新的表格。\n\n两种模式下的查询流程大致相同：选择子库 &gt;&gt; 选择时间范围 &gt;&gt; 选择代码范围 &gt;&gt; 选择字段范围 &gt;&gt; 下载数据。\n建议尽量选择 .csv 或 .txt 格式下载数据，因为这两种格式的数据读取速度更快。\n下载后的文件通常为 .zip 格式，解压后会得到两份文件： - FileName.csv: 数据文件 - FileName[DES][xlsx].txt: 变量说明文件\n数据库说明书：每个子库的右上角都会显示「数据库说明书」，点击后可下载 PDF 格式的说明书，里面包含了该子库的所有表格、字段的详细信息。\n\n\n# 文件路径设定{tag}\n# Note：进行后续分析之前，请先执行本 Cell 中的代码，以确保所有路径正确设置。\n\nimport os\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# 打开 .ipynb 文件的路径记为当前工作目录\n#path = os.getcwd()\npath = r'D:\\Github\\ds_data\\data\\CSMAR'    # 替换为你的实际路径\nos.chdir(path)  # 切换工作目录\nprint('Working Directory:', os.getcwd())\n\n\n# Folders \nzip_folder = os.path.join(path, 'data_raw_zip')       # 原始压缩包存放目录\nextract_folder = os.path.join(path, 'data_raw')       # 解压后的数据存放目录\ndata_clean_folder = os.path.join(path, 'data_clean')  # 清洗后的数据存放目录\n\nWorking Directory: d:\\Github\\ds_data\\data\\CSMAR\n\n\n\n# 列示 'data_raw_zip' 文件夹中的所有文件\nfor item in os.listdir(zip_folder):\n    print(item)\n\nCSMAR常用变量-2000-2024.zip\n上市公司基本信息变更表2000-2024.zip\n上市公司基本信息年度表.zip\n利润表-现金流量表-2000-2010.zip\n利润表-现金流量表-2011-2024.zip\n资产负债表-2000-2010.zip\n资产负债表-2011-2024.zip",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#文件夹结构与处理流程说明",
    "href": "examples/CSMAR/01_unzip_to_csv.html#文件夹结构与处理流程说明",
    "title": "13  CSMAR 数据处理",
    "section": "13.2 文件夹结构与处理流程说明",
    "text": "13.2 文件夹结构与处理流程说明\n本项目的数据处理涉及以下文件夹：\n\ndata_raw_zip：存放从 CSMAR 下载的原始数据（.zip 压缩包），每个压缩包解压后会生成同名文件夹。\ndata_raw：存放解压后的原始数据文件。\ndata_clean：存放清洗后的数据文件。\n\n\n13.2.1 处理流程\n\n列出 data_raw_zip 文件夹中的所有压缩文件。\n将每个压缩包解压到 data_raw 文件夹下，解压后以压缩包同名文件夹存放。\n为便于后续批量处理，对解压后的文件进行统一重命名。\n编写函数，实现指定文件夹下文件的批量读取。",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#解压-zip-文件到指定文件夹",
    "href": "examples/CSMAR/01_unzip_to_csv.html#解压-zip-文件到指定文件夹",
    "title": "13  CSMAR 数据处理",
    "section": "13.3 解压 zip 文件到指定文件夹",
    "text": "13.3 解压 zip 文件到指定文件夹\n\n\n\n\n\n\n提示词\n\n\n\n\n将当前路径下的 ‘data_raw_zip’ 文件夹中的所有 zip 文件解压到 ‘data_raw’ 文件夹中。\n如果 ‘data_raw’ 文件夹不存在，则创建该文件夹。\n每个 zip 文件解压后生成一个文件夹，文件夹名称与 zip 文件名相同，若有同名文件则覆盖之。\n\n\n\n\nimport zipfile\nimport shutil\n\n# 创建 'data_raw' 文件夹（如果不存在）\nif not os.path.exists(extract_folder):\n    os.makedirs(extract_folder)\n\n# 遍历 'data_raw_zip' 文件夹中的所有 zip 文件\nfor item in os.listdir(zip_folder):\n    if item.endswith('.zip'):\n        zip_path = os.path.join(zip_folder, item)\n        folder_name = os.path.splitext(item)[0]\n        target_dir = os.path.join(extract_folder, folder_name)\n        # 如果目标文件夹已存在，则先删除\n        if os.path.exists(target_dir):\n            shutil.rmtree(target_dir)\n        # 解压 zip 文件到目标文件夹\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(target_dir)\n        print(f\"已解压: {zip_path} -&gt; {target_dir}\")\n\n已解压: D:\\Github\\ds_data\\data\\CSMAR\\data_raw_zip\\CSMAR常用变量-2000-2024.zip -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\CSMAR常用变量-2000-2024\n已解压: D:\\Github\\ds_data\\data\\CSMAR\\data_raw_zip\\上市公司基本信息变更表2000-2024.zip -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\上市公司基本信息变更表2000-2024\n已解压: D:\\Github\\ds_data\\data\\CSMAR\\data_raw_zip\\上市公司基本信息年度表.zip -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\上市公司基本信息年度表\n已解压: D:\\Github\\ds_data\\data\\CSMAR\\data_raw_zip\\利润表-现金流量表-2000-2010.zip -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2000-2010\n已解压: D:\\Github\\ds_data\\data\\CSMAR\\data_raw_zip\\利润表-现金流量表-2011-2024.zip -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2011-2024\n已解压: D:\\Github\\ds_data\\data\\CSMAR\\data_raw_zip\\资产负债表-2000-2010.zip -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2000-2010\n已解压: D:\\Github\\ds_data\\data\\CSMAR\\data_raw_zip\\资产负债表-2011-2024.zip -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2011-2024",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#更改部分文件夹中的文件名",
    "href": "examples/CSMAR/01_unzip_to_csv.html#更改部分文件夹中的文件名",
    "title": "13  CSMAR 数据处理",
    "section": "13.4 更改部分文件夹中的文件名",
    "text": "13.4 更改部分文件夹中的文件名\n\n13.4.1 问题说明\n在 data_raw 文件夹中，有些子文件夹中的文件名是由 CSMAR 自动生成的，没有实际意义。为了便于后续处理，需要将这些文件名更改为更有意义的名称。\n此处以 data_raw/资产负债表-?? 文件夹为例，可以看出，资产负债表-2000-2010 和 资产负债表-2011-2024 这两个文件夹中包含的文件名完全相同，这会导致随后纵向合并时难以区分数据来源。\n\n# 提示词：列出 `data_raw` 文件夹中包含关键词 {'资产负债表'} 的子文件夹的 file tree。\n\nkeywords = '资产负债表'\n\ndef print_keyword_file_tree(root, keyword, indent=\"\"):\n    for item in os.listdir(root):\n        item_path = os.path.join(root, item)\n        if os.path.isdir(item_path) and keyword in item:\n            print(indent + \"|-- \" + item)\n            for subitem in os.listdir(item_path):\n                print(indent + \"    |-- \" + subitem)\n\nprint_keyword_file_tree(extract_folder, keywords)\n\n|-- 资产负债表-2000-2010\n    |-- 版权声明.pdf\n    |-- 跨表查询_沪深京股票(年频).xlsx\n    |-- 跨表查询_沪深京股票(年频)[DES][.xlsx].txt\n|-- 资产负债表-2011-2024\n    |-- 版权声明.pdf\n    |-- 跨表查询_沪深京股票(年频).xlsx\n    |-- 跨表查询_沪深京股票(年频)[DES][.xlsx].txt\n\n\n查看后，发现子文件夹 利润表-现金流量表-?? 和 CSMAR常用变量-2000-2024 中的文件名也存在类似问题。\n处理思路是：用文件夹名称作为该文件夹下的文件的文件名。因此，我们可以使用如下提示词生成处理代码：\n\n\n\n\n\n\n提示词\n\n\n\n\n目的：更改 ‘data_raw’ 文件夹中部分子文件夹中的文件的名称。\n子文件夹名称为 ‘sub_folder_name’，其内部包含的文件记为 {Files}\n如果 ‘sub_folder_name’ 中包含关键词 {‘常用变量’, ‘资产负债表’, ‘利润表’}，则把 {Files} 中的 ‘.xlsx’ 和 ‘.txt’ 文件的名称改为 ‘folder_name.xlsx’ 和 ‘folder_name_DES.txt’。\n\n\n\n\n# 增加关键词 '常用变量'\nkeywords = ['常用变量', '资产负债表', '利润表']\n\n# 检查并重命名文件\nfor subfolder in os.listdir(extract_folder):\n    subfolder_path = os.path.join(extract_folder, subfolder)\n    if os.path.isdir(subfolder_path) and any(kw in subfolder for kw in keywords):\n        for file in os.listdir(subfolder_path):\n            file_path = os.path.join(subfolder_path, file)\n            if file.endswith('.xlsx'):\n                new_name = f\"{subfolder}.xlsx\"\n                new_path = os.path.join(subfolder_path, new_name)\n                os.rename(file_path, new_path)\n                print(f\"重命名: {file_path} -&gt; {new_path}\")\n            elif file.endswith('.txt'):\n                new_name = f\"{subfolder}_DES.txt\"\n                new_path = os.path.join(subfolder_path, new_name)\n                os.rename(file_path, new_path)\n                print(f\"重命名: {file_path} -&gt; {new_path}\")\n\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\CSMAR常用变量-2000-2024\\常用变量查询（年度）.xlsx -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\CSMAR常用变量-2000-2024\\CSMAR常用变量-2000-2024.xlsx\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\CSMAR常用变量-2000-2024\\常用变量查询（年度）[DES][xlsx].txt -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\CSMAR常用变量-2000-2024\\CSMAR常用变量-2000-2024_DES.txt\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2000-2010\\跨表查询_沪深京股票(年频).xlsx -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2000-2010\\利润表-现金流量表-2000-2010.xlsx\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2000-2010\\跨表查询_沪深京股票(年频)[DES][.xlsx].txt -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2000-2010\\利润表-现金流量表-2000-2010_DES.txt\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2011-2024\\跨表查询_沪深京股票(年频).xlsx -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2011-2024\\利润表-现金流量表-2011-2024.xlsx\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2011-2024\\跨表查询_沪深京股票(年频)[DES][.xlsx].txt -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2011-2024\\利润表-现金流量表-2011-2024_DES.txt\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2000-2010\\跨表查询_沪深京股票(年频).xlsx -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2000-2010\\资产负债表-2000-2010.xlsx\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2000-2010\\跨表查询_沪深京股票(年频)[DES][.xlsx].txt -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2000-2010\\资产负债表-2000-2010_DES.txt\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2011-2024\\跨表查询_沪深京股票(年频).xlsx -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2011-2024\\资产负债表-2011-2024.xlsx\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2011-2024\\跨表查询_沪深京股票(年频)[DES][.xlsx].txt -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2011-2024\\资产负债表-2011-2024_DES.txt\n\n\n\n\n13.4.2 查看处理后的文件名\n\n# 提示词：列出 `data_raw` 文件夹中包含关键词 {'资产负债表'} 的子文件夹的 file tree。\n\nkeyword = '资产负债表'\n\ndef print_keyword_file_tree(root, keyword, indent=\"\"):\n    for item in os.listdir(root):\n        item_path = os.path.join(root, item)\n        if os.path.isdir(item_path) and keyword in item:\n            print(indent + \"|-- \" + item)\n            for subitem in os.listdir(item_path):\n                print(indent + \"    |-- \" + subitem)\n\nprint_keyword_file_tree(extract_folder, keyword)\n\n|-- 资产负债表-2000-2010\n    |-- 版权声明.pdf\n    |-- 资产负债表-2000-2010.xlsx\n    |-- 资产负债表-2000-2010_DES.txt\n|-- 资产负债表-2011-2024\n    |-- 版权声明.pdf\n    |-- 资产负债表-2011-2024.xlsx\n    |-- 资产负债表-2011-2024_DES.txt",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#列示-data_raw-文件夹中的-file-tree",
    "href": "examples/CSMAR/01_unzip_to_csv.html#列示-data_raw-文件夹中的-file-tree",
    "title": "13  CSMAR 数据处理",
    "section": "13.5 列示 ‘data_raw’ 文件夹中的 file tree",
    "text": "13.5 列示 ‘data_raw’ 文件夹中的 file tree\n\n\n\n\n\n\n提示词\n\n\n\n\n列示 ‘data_raw’ 文件夹中的文件树结构。\n只列示文件夹名称和文件名称，不需要显示文件内容。\n如果文件夹中有子文件夹，则显示子文件夹名称。\n\n\n\n\ndef print_file_tree(root, indent=\"\"):\n    for item in os.listdir(root):\n        item_path = os.path.join(root, item)\n        print(indent + \"|-- \" + item)\n        if os.path.isdir(item_path):\n            print_file_tree(item_path, indent + \"    \")\n\nprint_file_tree(extract_folder)\n\n|-- CSMAR常用变量-2000-2024\n    |-- CSMAR常用变量-2000-2024.xlsx\n    |-- CSMAR常用变量-2000-2024_DES.txt\n    |-- 版权声明.pdf\n|-- 上市公司基本信息变更表2000-2024\n    |-- STK_LISTEDCOINFOCHG.xlsx\n    |-- STK_LISTEDCOINFOCHG[DES][xlsx].txt\n    |-- 版权声明.pdf\n|-- 上市公司基本信息年度表\n    |-- STK_LISTEDCOINFOANL.xlsx\n    |-- STK_LISTEDCOINFOANL[DES][xlsx].txt\n    |-- 上市公司基本信息 数据库说明书.pdf\n|-- 利润表-现金流量表-2000-2010\n    |-- 利润表-现金流量表-2000-2010.xlsx\n    |-- 利润表-现金流量表-2000-2010_DES.txt\n    |-- 版权声明.pdf\n|-- 利润表-现金流量表-2011-2024\n    |-- 利润表-现金流量表-2011-2024.xlsx\n    |-- 利润表-现金流量表-2011-2024_DES.txt\n    |-- 版权声明.pdf\n|-- 资产负债表-2000-2010\n    |-- 版权声明.pdf\n    |-- 资产负债表-2000-2010.xlsx\n    |-- 资产负债表-2000-2010_DES.txt\n|-- 资产负债表-2011-2024\n    |-- 版权声明.pdf\n    |-- 资产负债表-2011-2024.xlsx\n    |-- 资产负债表-2011-2024_DES.txt",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#定义函数以批量读取指定文件夹下的文件",
    "href": "examples/CSMAR/01_unzip_to_csv.html#定义函数以批量读取指定文件夹下的文件",
    "title": "13  CSMAR 数据处理",
    "section": "13.6 定义函数以批量读取指定文件夹下的文件",
    "text": "13.6 定义函数以批量读取指定文件夹下的文件\n\n13.6.1 文件内容分析\n观察后发现，每个文件夹下都包含两个文件：\n\n*.xlsx：包含数据的 Excel 文件。\n*DES*.txt：包含数据描述的文本文件。\n\n我们可以查看这两份文件的内容和结构，然后再决定读取方案。这里仍以 [资产负债表-2011-2024] 文件夹为例进行说明。\n资产负债表-2011-2024.xlsx 是数据文件，存储结构如下：前 6 列和前 9 行数据如下：\n\n可以看出，从第 5 行开始是具体的观察值，而此前的四行则是表头信息：\n\n第一行是变量名\n第二行是变量的中文简称 (可以用作变量中文标签)\n第三行是变量的单位\n第四行是变量的其他说明信息。如第 5-6 列显示的是报表类型。\n\n资产负债表-2011-2024_DES.txt 是变量描述文件，主要包含变量的中文名称、单位、数据来源等信息。其原始内容如下：\n-------------------- 资产负债表-2011-2024_DES.txt ---------begin--\n本信息:\ncode[证券代码] - \nstknme[证券简称] - \nlistingDate[上市日期] - \nEndDate[时间] - \n\n数据库名称：财务报表--&gt; 表名称：资产负债表(FS_Combas)\nA001101000[货币资金] - 公司库存现金、银行结算户存款、……等的合计数。1990年起使用\nA001107000[交易性金融资产] - 交易性金融资产是……的债券投资、股票投资……。2007年起使用。\n……\n-------------------- 资产负债表-2011-2024_DES.txt ---------over--\n可以看出，资产负债表-2011-2024_DES.txt 文件中每行的格式为：\nvarname[中文简称] - 变量说明\n\nvarname 是变量的英文名称。\n中文简称 是变量的中文名称，可以作为变量标签。\n变量说明 是对该变量的详细描述，包括变量的含义、计算方法、单位、数据来源等信息。\n\n对比这两份文件，有如下几种处理方案：\n方案 1：简化版\n如果只需要 varname 和 中文简称 (后者可以作为前者的变量标签)，则处理过程为：\ninput：资产负债表-2011-2024.xlsx\n\n提取 资产负债表-2011-2024.xlsx 文件中第 5 行以后的所有数据，定义为数据框 df；\n提取 资产负债表-2011-2024.xlsx 前两行数据，定义为一个字典 dict - {varname: 中文简称}；\n将该字典附加到数据框 df 中，作为它的一个属性。\n\noutput：df 数据框 + dict 字典。\n这种处理方式的好处是只需要数据文件 资产负债表-2011-2024.xlsx，缺陷是使用数据时，如果想知道每个变量详细信息，就需要翻阅 CSMAR 提供的 PDF 说明书，或 资产负债表-2011-2024_DES.txt 文件。\n方案 2：完整信息版\n此版本的思路是把变量的中文名和变量说明信息都提取出来，制作成两个字典，整合到数据框中。其好处是，我们可以随时在 Python 内部查看变量的中文名称和说明信息，而不需要翻阅 PDF 或其他文件。\n处理过程如下：\ninput：资产负债表-2011-2024.xlsx + 资产负债表-2011-2024_DES.txt\n\n提取 资产负债表-2011-2024.xlsx 文件中第 5 行以后的所有数据，定义为数据框 df；\n从 资产负债表-2011-2024_DES.txt 文件中提取信息，定义两个字典：\n\n字典 1：{varname: 中文简称}，用于将英文变量名与其中文简称对应起来。\n字典 2：{varname: 变量说明}，用于将英文变量名与其详细说明对应起来。\n\n将这两个字典附加到数据框 df 中，作为它的两个属性。\n\noutput：df 数据框 + dict1 字典 + dict2 字典。\n方案 3：简化版-中文变量名\n如果已经对变量名有了充分了解，且只需要 中文简称，则处理过程为：\ninput：资产负债表-2011-2024.xlsx\n\n读入 资产负债表-2011-2024.xlsx 文件中第 2 行以后的所有数据，定义为数据框 df；\n删除数据框 df 中的第 2-3 行；\n\n上述方案的对比：\n\n方案 1：只保留英文变量名和中文简称，适用于对 CSMAR 数据库比较熟悉的用户。\n方案 2：保留英文变量名、中文简称和变量说明，适用于需要详细了解变量含义的用户。\n方案 3：只保留中文简称，变量含义直观明了。缺陷是，有些变量的中文简称中包含了 (，（， 等特殊字符，需要额外转换。因此，不太推荐这种方式。\n\n下面，我们将实现方案 1 的处理过程。\n\n\n13.6.2 方案 1：简化版处理代码\n处理思路：\n从 CSMAR 下载下来的每个 .zip 文件解压后，都会生成一个同名文件夹 (记为 'folder_path')。该文件夹下包含一个 .xlsx 格式的数据文件和一个 .txt 格式的描述文件。因此，我们只需要指定 'folder_path'，便可以让 Python 根据文件后缀自动识别并读取这两个文件。对于 方案 1，我们只需要读取 .xlsx 文件，并将该文件的第二行作为变量 (第一行) 的中文标签即可。\n\n\n\n\n\n\n提示词\n\n\n\n\n目的：读取指定文件夹下的 Excel 文件，并将其第二行作为变量的中文标签。\ninput：folder = ‘资产负债表-2011-2024’\n\nfolder_path = ‘{path}/data_raw/{folder}’\n{path} 已经在第一个 cell (‘文件路径设定{tag}’) 中定义\n\n处理思路：\n\n检查该文件夹下以 .xlsx 结尾的文件的个数，如果大于 1 个，则报错。\n读入该文件夹下以 .xlsx 结尾的文件。完整文件名为：{filename}.xlsx。\n存入数据框 df_filename。\n\n打印 forder_path 和 df_filename 的名称\n\n删除 Excel 表格第 3 行和第 4 行\n做一个字典：dict_filename，键为英文变量名，值为中文变量名。\n\nkey: 英文变量名，从 Excel 第 1 行提取。\nvalue: 中文变量名，从 Excel 第 2 行提取。\n\n将 dict_filename 附加到 df_filename 数据框中，作为它的一个属性。\n删除第 2 行\n\n显示处理后的数据框 df_filename.head() 的前 5 列和字典 dict_filename 的全部 {key: value}。\n\n\n\n\nimport pandas as pd\n\n# 构造文件夹路径\nfolder = '资产负债表-2011-2024'\nfolder_path = os.path.join(path, 'data_raw', folder)\n\n# 检查 .xlsx 文件数量，排除临时文件（如~$开头的文件）\nxlsx_files = [f for f in os.listdir(folder_path) if f.endswith('.xlsx') and not f.startswith('~$')]\nif len(xlsx_files) != 1:\n    raise ValueError(f\"{folder_path} 下存在 {len(xlsx_files)} 个有效 .xlsx 文件，请检查！\")\nxlsx_file = xlsx_files[0]\nxlsx_path = os.path.join(folder_path, xlsx_file)\n\nprint(f\"folder_path: {folder_path}\")\nprint(f\"df_filename: {xlsx_file}\")\nprint(' ')\n\n# 读取前4行\nheader = pd.read_excel(xlsx_path, nrows=4, header=None)\n\n# 删除第3、4行（索引2、3），再删除第2行（索引1），只保留第0行\nheader_clean = header.drop([2, 3]).reset_index(drop=True)\nheader_clean = header_clean.drop([1]).reset_index(drop=True)\n\n# 提取英文变量名和中文变量名\nen_names = header.iloc[0]\ncn_names = header.iloc[1]\ndict_filename = dict(zip(en_names, cn_names))\n\n# 读取数据，跳过前4行\ndf_filename = pd.read_excel(xlsx_path, skiprows=4, header=None)\ndf_filename.columns = en_names\n\n# 附加字典为属性\ndf_filename.varname_cn = dict_filename\n\n# 显示前5列\nprint(df_filename.iloc[:, :5].head())\n# 显示字典的前10个键值对\nfor k, v in list(dict_filename.items())[:10]:\n    print(f\"{k}: {v}\")\n\nfolder_path: d:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2011-2024\ndf_filename: 资产负债表-2011-2024.xlsx\n \n0  code  stknme listingDate  EndDate  FS_Combas-A001101000\n0     1    平安银行  1991-04-03     2011          0.000000e+00\n1     2     万科A  1991-01-29     2011          3.423951e+10\n2     3  PT 金田A  1991-07-03     2011                   NaN\n3     4  *ST 国华  1991-01-14     2011          5.712837e+07\n4     5   ST 星源  1990-12-10     2011          1.629275e+07\ncode: 证券代码\nstknme: 证券简称\nlistingDate: 上市日期\nEndDate: 时间\nFS_Combas-A001101000: 货币资金\nFS_Combas-A001107000: 交易性金融资产\nFS_Combas-A001109000: 短期投资净额\nFS_Combas-A001123000: 存货净额\nFS_Combas-A001100000: 流动资产合计\nFS_Combas-A001212000: 固定资产净额\n\n\n\n\n13.6.3 优化代码\n上述代码可以一次性顺利执行，但耗时为 53s，太慢。于是，我把上述提示词和代码发给了 ChatGPT，请它优化代码。它给出的建议是：pandas 读取 .xlsx 格式本身就比较慢，尤其是数据量大或格式复杂时，速度瓶颈主要在于底层解析 Excel 文件的过程。此外，上述代码读取了两次 Excel 文件，第一次读取是为了获取变量名和中文简称，第二次读取是为了获取数据，这样会导致重复的 I/O 操作，进一步降低速度。\n应对方法是，先使用 xlsx2csv 包将 .xlsx 文件快速转换为 .csv 格式，然后再用 pandas 的 read_csv 读取数据，这样可以极大提升读取速度。\n优化后的代码只需 1-2 秒即可完成 (ChatGPT 提示词)：\n\n\n\n\n\n\n提示词\n\n\n\n目标：定义一个函数 read_files_in_folder(folder_path, Fname=None)，用于批量读取指定文件夹下的 .xlsx 文件，并返回数据框和变量名-中文名字典。\n要求：\n\n参数说明：\n\nfolder_path：字符串，指定要读取的文件夹路径。\nFname：字符串，可选，指定数据框和字典的名称前缀。若为 None，则自动取 .xlsx 文件名（不含扩展名），并将特殊字符替换为下划线。\n\n处理流程：\n\n检查文件夹下以 .xlsx 结尾且不以 ~$ 开头的文件，若数量不为 1，则报错。\n将 .xlsx 文件转换为 .csv 文件（如已存在则跳过），转换时编码为 gbk。\n读取 .csv 文件前 4 行，提取英文变量名和中文变量名。\n对英文变量名，若为 ‘A-B’ 结构，则只保留 B 部分。\n构建 {英文变量名: 中文变量名} 的字典。\n跳过前 4 行读取正文数据，列名用处理后的英文变量名。\n返回一个 dict，包含数据框（键名为 df_{Fname}）和变量名字典（键名为 dic_{Fname}）。\n\n代码需包含必要的 import，且不重复导入已在 notebook 其他 cell 导入的模块。\n代码块必须完整、可直接运行。\n\n\n\n\n# {tag}: 'fcn_read_files_in_folder'\n#%pip install xlsx2csv\n\nimport os\nimport pandas as pd\nfrom xlsx2csv import Xlsx2csv\n\nfolder = '资产负债表-2011-2024'\nfolder_path = os.path.join(path, 'data_raw', folder)\n# ========================================\n\n# 检查文件夹下 .xlsx 文件\nxlsx_files = [f for f in os.listdir(folder_path) if f.endswith('.xlsx') and not f.startswith('~$')]\nif len(xlsx_files) != 1:\n    raise ValueError(f\"{folder_path} 下存在 {len(xlsx_files)} 个有效 .xlsx 文件，请检查！\")\nxlsx_file = xlsx_files[0]\nxlsx_path = os.path.join(folder_path, xlsx_file)\n\nprint('\\n', '-'*20, '文件存储路径和文件名:', '-'*20)\nprint(f\"folder_path: {folder_path}\")\nprint(f\"df_filename: {xlsx_file}\\n\")\n\n# 自动生成 CSV 文件名\ncsv_path = xlsx_path.replace('.xlsx', '.csv')\n# 如已存在则跳过，否则转换\nif not os.path.exists(csv_path):\n    Xlsx2csv(xlsx_path, outputencoding=\"gbk\").convert(csv_path)\n\n# 读取前4行\nheader = pd.read_csv(csv_path, nrows=4, header=None)\n\n# 提取变量名\nen_names = header.iloc[0].tolist()\ncn_names = header.iloc[1].tolist()\ndict_filename = dict(zip(en_names, cn_names))\n\n# 跳过前4行读取正文\ndf_filename = pd.read_csv(\n    csv_path, skiprows=4, header=None, names=en_names, dtype=str\n)\n\n# 附加字典为属性\nsetattr(df_filename, 'varname_cn', dict_filename)\n\n# 显示前5列\nprint(df_filename.iloc[:, :5].head())\nprint('\\n\\n', '-'*20, '变量名字典:', '-'*20)\nfor k, v in list(dict_filename.items())[:10]:\n    print(f\"{k}: {v}\")\nprint('\\n\\n', '-'*20, '数据框维度:', '-'*20)\nprint(df_filename.shape)\n\n\n -------------------- 文件存储路径和文件名: --------------------\nfolder_path: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2011-2024\ndf_filename: 资产负债表-2011-2024.xlsx\n\n     code  stknme listingDate EndDate FS_Combas-A001101000\n0  000001    平安银行  1991-04-03    2011                    0\n1  000002     万科A  1991-01-29    2011   34239514295.080002\n2  000003  PT 金田A  1991-07-03    2011                  NaN\n3  000004  *ST 国华  1991-01-14    2011      57128374.050000\n4  000005   ST 星源  1990-12-10    2011      16292748.160000\n\n\n -------------------- 变量名字典: --------------------\ncode: 证券代码\nstknme: 证券简称\nlistingDate: 上市日期\nEndDate: 时间\nFS_Combas-A001101000: 货币资金\nFS_Combas-A001107000: 交易性金融资产\nFS_Combas-A001109000: 短期投资净额\nFS_Combas-A001123000: 存货净额\nFS_Combas-A001100000: 流动资产合计\nFS_Combas-A001212000: 固定资产净额\n\n\n -------------------- 数据框维度: --------------------\n(81662, 32)",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#定义函数以批量读取指定文件夹下的文件-1",
    "href": "examples/CSMAR/01_unzip_to_csv.html#定义函数以批量读取指定文件夹下的文件-1",
    "title": "13  CSMAR 数据处理",
    "section": "13.7 定义函数以批量读取指定文件夹下的文件",
    "text": "13.7 定义函数以批量读取指定文件夹下的文件\n几个要点：\n\n这个提示词还需修改：目前的提示词不具有独立性，应该与上一个 Cell 的提示词合并。\n变量名的处理：在 ‘利润表-现金流量表xxx.csv’ 和 ‘资产负债表xxx.csv’ 文件中，变量名的格式为 ‘A-B’，我们需要删除 ‘A-’ 部分，只保留 ‘C’ 部分。\n\nFS_Combas-A001101000: 货币资金 变为 A001101000: 货币资金\nFS_Comins-B003000000: 基本每股收益 变为 B003000000: 基本每股收益\nFS_Comscfd-C001021000: 支付的各项税费 变为 C001021000: 支付的各项税费\n\n\n\n\n\n\n\n\n提示词\n\n\n\n\n以 {tag}: ‘read_xlsx_example’ Cell 中的代码为基础，定义一个函数 read_files_in_folder(folder_path, Fname)，用于批量读取指定文件夹下的所有文件。\n\n函数参数 folder_path 是一个字符串，表示要读取的文件夹路径。\nFname 是一个字符串，表示文件夹的名称，用于生成数据框和字典的名称。如果用户不指定，则默认使用 .xlsx 文件的名称，特殊字符采用 _ 替换，确保符合 Python 命名规则。\n如果变量名为 A-B 结构，则删除 ‘A-’ 部分，只保留 ‘B’ 部分。\n\n函数返回：一个数据框，名称为 df_{Fname}；一个字典，用于存储 {变量名: 变量中文简称}，名称为 dic_{Fname}。\n\n\n\n\nimport os\nimport pandas as pd\nimport re\nfrom xlsx2csv import Xlsx2csv\n\ndef read_files_in_folder(folder_path, Fname=None):\n    \"\"\"\n    批量读取指定文件夹下的 .xlsx 文件，返回数据框和变量名-中文名字典。\n    参数:\n        folder_path: 文件夹路径\n        Fname: 文件夹名称（可选），用于生成数据框和字典的名称。若为 None，则自动取 .xlsx 文件名（不含扩展名），并将特殊字符替换为下划线。\n    返回:\n        df_{Fname}: 数据框\n        dic_{Fname}: {英文变量名: 中文变量名} 字典\n    \"\"\"\n    # 查找 .xlsx 文件\n    xlsx_files = [f for f in os.listdir(folder_path) if f.endswith('.xlsx') and not f.startswith('~$')]\n    if len(xlsx_files) != 1:\n        raise ValueError(f\"{folder_path} 下存在 {len(xlsx_files)} 个有效 .xlsx 文件，请检查！\")\n    xlsx_file = xlsx_files[0]\n    xlsx_path = os.path.join(folder_path, xlsx_file)\n\n    # 自动生成 CSV 文件名\n    csv_path = xlsx_path.replace('.xlsx', '.csv')\n    if not os.path.exists(csv_path):\n        Xlsx2csv(xlsx_path, outputencoding=\"gbk\").convert(csv_path)  # 若使用 'utf-8' encoding，可能会导致中文乱码问题\n\n    # 读取前4行\n    header = pd.read_csv(csv_path, nrows=4, header=None)\n    en_names = header.iloc[0].tolist()\n    cn_names = header.iloc[1].tolist()\n\n    # 变量名处理：如果为 'A-B' 结构，则只保留 B 部分\n    def clean_varname(name):\n        if isinstance(name, str) and '-' in name:\n            parts = name.split('-')\n            if len(parts) &gt;= 2:\n                return parts[-1]\n        return name\n\n    en_names_clean = [clean_varname(n) for n in en_names]\n    dic = dict(zip(en_names_clean, cn_names))\n\n    # 跳过前4行读取正文\n    df = pd.read_csv(csv_path, skiprows=4, header=None, names=en_names_clean, dtype=str)\n\n    # 生成 Fname\n    if Fname is None:\n        Fname = os.path.splitext(xlsx_file)[0]\n    Fname = re.sub(r'\\W+', '_', Fname)\n\n    # 返回带有指定名称的数据框和字典\n    return {f'df_{Fname}': df, f'dic_{Fname}': dic}",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#批量处理",
    "href": "examples/CSMAR/01_unzip_to_csv.html#批量处理",
    "title": "13  CSMAR 数据处理",
    "section": "13.8 批量处理",
    "text": "13.8 批量处理\n接下来，我们就可以用上面定义的函数来批量处理 data_raw 文件夹中的所有子文件夹了。\n\n\n\n\n\n\n提示词\n\n\n\n用 {tag}: ‘fcn_read_files_in_folder’ Cell 中定义的 read_files_in_folder() 函数批量读取 data_raw 文件夹中的所有子文件夹下的文件，并将结果存储到 data_clean 文件夹中。主要步骤如下：\n\n如果 data_clean 文件夹不存在，则创建该文件夹；如果已经存在，则清空该文件夹中的所有内容。\n遍历 data_raw 文件夹中的所有子文件夹。\n\n对于每个子文件夹，调用 read_files_in_folder 函数，读取其中的 .xlsx 文件和 .txt 文件。\n将读取到的数据框和字典保存到 data_clean 文件夹中，文件名为 {子文件夹名称}.pkl。\n\n\n\n\n\nimport shutil\nimport pickle\n\n# 1. 如果 data_clean_folder 不存在则创建，已存在则清空\nif os.path.exists(data_clean_folder):\n    shutil.rmtree(data_clean_folder)\nos.makedirs(data_clean_folder)\n\n# 2. 遍历 data_raw 文件夹下所有子文件夹\nfor subfolder in os.listdir(extract_folder):\n    subfolder_path = os.path.join(extract_folder, subfolder)\n    if os.path.isdir(subfolder_path):\n        try:\n            # 读取数据和变量名字典\n            result = read_files_in_folder(subfolder_path, Fname=subfolder)\n            # 统一保存为 {'df': df, 'varname_cn': dic}\n            df_key = [k for k in result if k.startswith('df_')][0]\n            dic_key = [k for k in result if k.startswith('dic_')][0]\n            save_dict = {'df': result[df_key], 'varname_cn': result[dic_key]}\n            # 保存为 pkl 文件\n            save_path = os.path.join(data_clean_folder, f\"{subfolder}.pkl\")\n            with open(save_path, 'wb') as f:\n                pickle.dump(save_dict, f)\n            print(f\"已保存: {save_path}\")\n        except Exception as e:\n            print(f\"处理 {subfolder} 时出错: {e}\")\n\n处理 CSMAR常用变量-2000-2024 时出错: 'utf-8' codec can't decode byte 0xb9 in position 457: invalid start byte\n处理 上市公司基本信息变更表2000-2024 时出错: 'gbk' codec can't encode character '\\xa0' in position 93: illegal multibyte sequence\n处理 上市公司基本信息年度表 时出错: 'gbk' codec can't encode character '\\ufeff' in position 188: illegal multibyte sequence\n处理 利润表-现金流量表-2000-2010 时出错: 'utf-8' codec can't decode byte 0xb4 in position 728: invalid start byte\n处理 利润表-现金流量表-2011-2024 时出错: 'utf-8' codec can't decode byte 0xb4 in position 728: invalid start byte\n处理 资产负债表-2000-2010 时出错: 'utf-8' codec can't decode byte 0xb4 in position 624: invalid start byte\n处理 资产负债表-2011-2024 时出错: 'utf-8' codec can't decode byte 0xb4 in position 624: invalid start byte\n\n\n\n13.8.1 查看导入结果\n\n\n\n\n\n\n提示词\n\n\n\n列出 data_clean 文件夹中的所有文件，以及每个文件的对应字典的前五个 {key: value}\n\n\n\nimport os\nimport pickle\n\n# 列出 data_clean 文件夹中每个文件的如下信息：\n#   - 文件名\n#   - 数据框的维度以及 '前五行+前五列'\n#   - 字典的前五个 {key: value} 对\n\nprint(\"\\n&gt;&gt;&gt;&gt; 当前 data_clean 文件夹中的文件：\")\nfor item in os.listdir(data_clean_folder):\n    if item.endswith('.pkl'):\n        print('\\n','-'*10, item, '-'*10)\n        with open(os.path.join(data_clean_folder, item), 'rb') as f:\n            data = pickle.load(f)\n            if isinstance(data, dict) and 'df' in data and 'varname_cn' in data:\n                df = data['df']\n                var_dict = data['varname_cn']\n                print(f\"数据框 shape: {df.shape}\")\n                print(\"前五行前五列：\")\n                print(df.iloc[:5, :5])\n                print(\"前五个变量名-中文名：\")\n                for k, v in list(var_dict.items())[:5]:\n                    print(f\"  {k}: {v}\")\n\n\n&gt;&gt;&gt;&gt; 当前 data_clean 文件夹中的文件：\n\n ---------- CSMAR常用变量-2000-2024.pkl ----------\n数据框 shape: (61455, 33)\n前五行前五列：\n    Stkcd accper stknme AnaAttention       Audittyp\n0  000001   2001   平安银行          NaN        标准无保留意见\n1  000001   2002   平安银行     1.098612        标准无保留意见\n2  000001   2003   平安银行     1.386294        标准无保留意见\n3  000001   2004   平安银行     1.791759  带有解释性说明的无保留意见\n4  000001   2005   平安银行     1.791759        标准无保留意见\n前五个变量名-中文名：\n  Stkcd: 股票代码\n  accper: 会计年度\n  stknme: 股票简称\n  AnaAttention: 分析师关注度\n  Audittyp: 审计意见\n\n ---------- 上市公司基本信息变更表2000-2024.pkl ----------\n数据框 shape: (160275, 8)\n前五行前五列：\n   Symbol AnnouncementDate ImplementDate ChangedItem    SecurityID\n0  000001              NaN    1991-04-03        所属省份  201000000001\n1  000001              NaN    1991-04-03        注册地址  201000000001\n2  000001              NaN    1991-04-03        公司全称  201000000001\n3  000001              NaN    1991-04-03        证券简称  201000000001\n4  000001              NaN    1991-04-03      公司经营性质  201000000001\n前五个变量名-中文名：\n  Symbol: 股票代码\n  AnnouncementDate: 公告日期\n  ImplementDate: 实施日期\n  ChangedItem: 变更属性\n  SecurityID: 证券ID\n\n ---------- 上市公司基本信息年度表.pkl ----------\n数据框 shape: (64170, 40)\n前五行前五列：\n   Symbol ShortName     EndDate ListedCoID    SecurityID\n0  000001      深发展A  2001-12-31     101704  201000000001\n1  000001      深发展A  2002-12-31     101704  201000000001\n2  000001      深发展A  2003-12-31     101704  201000000001\n3  000001      深发展A  2004-12-31     101704  201000000001\n4  000001      深发展A  2005-12-31     101704  201000000001\n前五个变量名-中文名：\n  Symbol: 股票代码\n  ShortName: 股票简称\n  EndDate: 统计截止日期\n  ListedCoID: 上市公司ID\n  SecurityID: 证券ID\n\n ---------- 利润表-现金流量表-2000-2010.pkl ----------\n数据框 shape: (64163, 36)\n前五行前五列：\n     code  stknme listingDate EndDate         B001101000\n0  000001    平安银行  1991-04-03    2000                NaN\n1  000002     万科A  1991-01-29    2000  3783668674.180000\n2  000003  PT 金田A  1991-07-03    2000   464723527.060000\n3  000004  *ST 国华  1991-01-14    2000   131006632.910000\n4  000005   ST 星源  1990-12-10    2000   145947499.350000\n前五个变量名-中文名：\n  code: 证券代码\n  stknme: 证券简称\n  listingDate: 上市日期\n  EndDate: 时间\n  B001101000: 营业收入\n\n ---------- 利润表-现金流量表-2011-2024.pkl ----------\n数据框 shape: (81662, 36)\n前五行前五列：\n     code  stknme listingDate EndDate          B001101000\n0  000001    平安银行  1991-04-03    2011                 NaN\n1  000002     万科A  1991-01-29    2011  71782749800.679993\n2  000003  PT 金田A  1991-07-03    2011                 NaN\n3  000004  *ST 国华  1991-01-14    2011     74503718.530000\n4  000005   ST 星源  1990-12-10    2011     63534839.010000\n前五个变量名-中文名：\n  code: 证券代码\n  stknme: 证券简称\n  listingDate: 上市日期\n  EndDate: 时间\n  B001101000: 营业收入\n\n ---------- 资产负债表-2000-2010.pkl ----------\n数据框 shape: (64163, 32)\n前五行前五列：\n     code  stknme listingDate EndDate        A001101000\n0  000001    平安银行  1991-04-03    2000               NaN\n1  000002     万科A  1991-01-29    2000  995745160.050000\n2  000003  PT 金田A  1991-07-03    2000   58018167.850000\n3  000004  *ST 国华  1991-01-14    2000   64780229.730000\n4  000005   ST 星源  1990-12-10    2000   29118049.740000\n前五个变量名-中文名：\n  code: 证券代码\n  stknme: 证券简称\n  listingDate: 上市日期\n  EndDate: 时间\n  A001101000: 货币资金\n\n ---------- 资产负债表-2011-2024.pkl ----------\n数据框 shape: (81662, 32)\n前五行前五列：\n     code  stknme listingDate EndDate          A001101000\n0  000001    平安银行  1991-04-03    2011                   0\n1  000002     万科A  1991-01-29    2011  34239514295.080002\n2  000003  PT 金田A  1991-07-03    2011                 NaN\n3  000004  *ST 国华  1991-01-14    2011     57128374.050000\n4  000005   ST 星源  1990-12-10    2011     16292748.160000\n前五个变量名-中文名：\n  code: 证券代码\n  stknme: 证券简称\n  listingDate: 上市日期\n  EndDate: 时间\n  A001101000: 货币资金\n\n\n简要分析：\n考虑到随后要将这些文件合并为一个数据框，我们需要重点关注如下几点：\n\n上市公司基本信息变更表2000-2024.pkl 暂时用不到，不予处理。\n哪些文件需要纵向合并 (append)?\n\n利润表-现金流量表-xxx.pkl 和 资产负债表-xxx.pkl 这两类文件需要纵向合并，分别存入数据框 df_profit 和 df_asset 中。\n\n哪些文件需要横向合并 (merge)?\n\n将 df_profit 和 df_asset 横向合并为一个数据框 df_financial；\n进而与 CSMAR常用变量-2000-2024.pkl 和 上市公司基本信息年度表.pkl 横向合并，得到最终的数据框 df_final。\n\n横向合并时，需要保证两份数据中有相同的变量名 (keys)。由于我们要合并的数据都是 ‘firm-year’ 格式的面板数据，因此需要保证每个数据框中都有 ‘code’ 和 ‘year’ 这两个变量。\n\n我们需要为 CSMAR常用变量-2000-2024.pkl 和 上市公司基本信息年度表.pkl 这两份文件添加 ‘code’ 和 ‘year’ 变量。二者分别对应这三个数据文件中的 Symbol 和 EndDate 变量。\n对于 利润表-现金流量表-xxx.pkl 和 资产负债表-xxx 文件，需要将 EndDate 变量转换为 year 变量，以便后续合并。\n\n\n\n\n13.8.2 查看处理后的单个文件\n\n\n\n\n\n\n提示词\n\n\n\n\n列示 ‘CSMAR常用变量-2000-2024.pkl’ 中的所有对象；\n空一行+分隔线；\n列示 ‘CSMAR常用变量-2000-2024.pkl’ 的 shape；\n空一行+分隔线；\n列示 ‘CSMAR常用变量-2000-2024.pkl’ 数据框中的 ‘前五行+前五列’；\n空一行+分隔线；\n列示字典中的所有 {变量名：中文简称}。\n\n\n\n\nimport pickle\n\n# 载入数据\npkl_path = os.path.join(data_clean_folder, 'CSMAR常用变量-2000-2024.pkl')\nwith open(pkl_path, 'rb') as f:\n    data = pickle.load(f)\n\n# 列示所有对象\nprint(\"对象列表：\", list(data.keys()))\n\nprint('\\n' + '-'*40 + '\\n')\n\n# 列示 shape\ndf_csmar = data['df']\nprint(\"数据框 shape:\", df_csmar.shape)\n\nprint('\\n' + '-'*40 + '\\n')\n\n# 列示前五行前五列\nprint(df_csmar.iloc[:5, :5])\n\nprint('\\n' + '-'*40 + '\\n')\n\n# 列示字典中的所有 {变量名：中文简称}\nvarname_cn_csmar = data['varname_cn']\nfor k, v in varname_cn_csmar.items():\n    print(f\"{k}: {v}\")\n\n对象列表： ['df', 'varname_cn']\n\n----------------------------------------\n\n数据框 shape: (61455, 33)\n\n----------------------------------------\n\n    Stkcd accper stknme AnaAttention       Audittyp\n0  000001   2001   平安银行          NaN        标准无保留意见\n1  000001   2002   平安银行     1.098612        标准无保留意见\n2  000001   2003   平安银行     1.386294        标准无保留意见\n3  000001   2004   平安银行     1.791759  带有解释性说明的无保留意见\n4  000001   2005   平安银行     1.791759        标准无保留意见\n\n----------------------------------------\n\nStkcd: 股票代码\naccper: 会计年度\nstknme: 股票简称\nAnaAttention: 分析师关注度\nAudittyp: 审计意见\nInternationalBig4: 审计师是否来自国际四大\nYsmvosd: 年个股流通市值\nYsmvttl: 年个股总市值\nYretwd: 考虑现金红利再投资的年个股回报率\nPropertyRightsNature: 产权性质\nSeperation: 两权分离度\nActualControllerNatureID: 实际控制人性质编码\nOwnershipProportion: 实际控制人拥有上市公司所有权比例\nControlProportion: 实际控制人拥有上市公司控制权比例\nShrcr1: 股权集中度1\nShrhfd5: 股权集中度9\nShrz: 股权集中度5\nFundHoldProportion: 基金持股比例\nQFIIHoldProportion: 合格境外投资者持股比例\nBrokerHoldProportion: 券商持股比例\nBankHoldProportion: 银行持股比例\nNonFinanceHoldProportion: 非金融类上市公司持股比例\nInsInvestorProp: 机构投资者持股比例\nStaffNumber: 员工人数\nConcurrentPosition: 两职合一\nBoardsize2: 董事会规模A\nExecutivesNumber: 高管人数\nIndDirector: 独立董事人数\nSumSalary: 管理层薪酬\nTOP3SumSalary: 高管前三名薪酬总额\nYnshrtrd: 年个股交易股数\nDirectorHoldshares: 董事会持股数量\nManageHoldshares: 高级管理人员持股数量",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#合并数据",
    "href": "examples/CSMAR/01_unzip_to_csv.html#合并数据",
    "title": "13  CSMAR 数据处理",
    "section": "13.9 合并数据",
    "text": "13.9 合并数据\n\n13.9.1 纵向合并\n\n\n\n\n\n\n提示词\n\n\n\n== 纵向合并不同年度的同名文件\n\n纵向合并 data_clean 文件夹中所有以 ‘资产负债表’ 开头的文件。\n\n将 EndDate 列重命名为 year 列。\n合并后的数据框命名为 df_assets，并保存到 data_clean 文件夹中，文件名为 df_assets.pkl。\n\n纵向合并 data_clean 文件夹中所有以 ‘利润表’ 开头的文件。\n\n将 EndDate 列重命名为 year 列。\n合并后的数据框命名为 df_profit，并保存到 data_clean 文件夹中，文件名为 df_profit.pkl。\n\n上述合并完成后，打印合并后的数据框的 shape；展示 ‘前三行 + 前五列’。\n\n\n\n\nimport os\nimport pickle\nimport pandas as pd\n\n# 合并资产负债表\nassets_dfs = []\nfor item in os.listdir(data_clean_folder):\n    if item.startswith('资产负债表') and item.endswith('.pkl'):\n        with open(os.path.join(data_clean_folder, item), 'rb') as f:\n            data = pickle.load(f)\n            df = data['df'] if isinstance(data, dict) and 'df' in data else data\n            if 'EndDate' in df.columns:\n                df = df.rename(columns={'EndDate': 'year'})\n            assets_dfs.append(df)\nif assets_dfs:\n    df_assets = pd.concat(assets_dfs, axis=0, ignore_index=True)\n    with open(os.path.join(data_clean_folder, 'df_assets.pkl'), 'wb') as f:\n        pickle.dump(df_assets, f)\n    print(\"df_assets shape:\", df_assets.shape)\n    print(df_assets.iloc[:3, :5])\n\nprint('\\n' + '-'*40 + '\\n')\n\n# 合并利润表\nprofit_dfs = []\nfor item in os.listdir(data_clean_folder):\n    if item.startswith('利润表') and item.endswith('.pkl'):\n        with open(os.path.join(data_clean_folder, item), 'rb') as f:\n            data = pickle.load(f)\n            df = data['df'] if isinstance(data, dict) and 'df' in data else data\n            if 'EndDate' in df.columns:\n                df = df.rename(columns={'EndDate': 'year'})\n            profit_dfs.append(df)\nif profit_dfs:\n    df_profit = pd.concat(profit_dfs, axis=0, ignore_index=True)\n    with open(os.path.join(data_clean_folder, 'df_profit.pkl'), 'wb') as f:\n        pickle.dump(df_profit, f)\n    print(\"df_profit shape:\", df_profit.shape)\n    print(df_profit.iloc[:3, :5])\n\ndf_assets shape: (145825, 32)\n     code  stknme listingDate  year        A001101000\n0  000001    平安银行  1991-04-03  2000               NaN\n1  000002     万科A  1991-01-29  2000  995745160.050000\n2  000003  PT 金田A  1991-07-03  2000   58018167.850000\n\n----------------------------------------\n\ndf_profit shape: (145825, 36)\n     code  stknme listingDate  year         B001101000\n0  000001    平安银行  1991-04-03  2000                NaN\n1  000002     万科A  1991-01-29  2000  3783668674.180000\n2  000003  PT 金田A  1991-07-03  2000   464723527.060000\n\n\n\n\n13.9.2 横向合并\n\n\n\n\n\n\n提示词\n\n\n\n== 横向合并不同来源的数据\n\n一些出现两次以上的功能，可以预先定义函数，确保代码结构清晰\n资产负债表与利润表的合并\n\n\n读入 data_clean 文件夹中的 df_assets.pkl。\n按照关键词 ‘{code, year}’ 与 df_profit.pkl 数据框横向合并；\n\n如果有同名变量，则自动忽略。\n\n合并后的数据存入数据框 df_financial;\n数据框的前两列变量为 ‘code, year’\n\n\n继续与 上市公司基本信息年度表, CSMAR常用变量-2000-2024.pkl 数据框横向合并。\n\n\n合并前，先将 上市公司基本信息年度表.pkl 中的 Symbol 列重命名为 code，EndDate 列重命名为 year。\n按照关键词 ‘{code, year}’ 与 df_financial 数据框横向合并；\n\n如果有同名变量，则自动忽略。\n\n\n\n输出合并后的数据文件\n\n\n合并后的数据存入数据框 df_final，\n\n第一列为 ‘code’，第二列为 ‘year’，是两个独立的变量。\n保存到 data_clean 文件夹中，文件名为 df_final.pkl。\n\n最终数据框 df_final 的前两列变量为 ‘code, year’，并展示其 shape 和 ‘前三行 + 前五列’。\n\n\n合并数据字典。将上述文件的字典合并为一个大字典 dict_CSMAR。\n\n\n同名变量的中文简称和说明信息取先进入的文件的。\n\n\n打印数据字典尺寸，以及前十行信息，格式：{变量名: 中文简称}；\n每组打印结果之间空一行，添加标题文字和分割线\n\n\n\n\nimport os\nimport pickle\nimport pandas as pd\n\n# 工具函数：按 code, year 横向合并，自动忽略同名变量\ndef merge_on_code_year(left, right, suffix='_right'):\n    # 只保留右表中不与左表重复的列\n    overlap = set(left.columns) & set(right.columns)\n    overlap -= {'code', 'year'}\n    right_use = right.drop(columns=list(overlap), errors='ignore')\n    merged = pd.merge(left, right_use, on=['code', 'year'], how='left', suffixes=('', suffix))\n    return merged\n\n# 1. 资产负债表与利润表的合并\nwith open(os.path.join(data_clean_folder, 'df_assets.pkl'), 'rb') as f:\n    df_assets = pickle.load(f)\nwith open(os.path.join(data_clean_folder, 'df_profit.pkl'), 'rb') as f:\n    df_profit = pickle.load(f)\n\n# 确保 'code' 和 'year' 列为字符串类型\ndf_assets['code'] = df_assets['code'].astype(str)\ndf_assets['year'] = df_assets['year'].astype(str)\ndf_profit['code'] = df_profit['code'].astype(str)\ndf_profit['year'] = df_profit['year'].astype(str)\n\ndf_financial = merge_on_code_year(df_assets, df_profit, suffix='_profit')\n\n# 2. 继续与 “上市公司基本信息年度表” 和 “CSMAR常用变量-2000-2024.pkl” 合并\nwith open(os.path.join(data_clean_folder, '上市公司基本信息年度表.pkl'), 'rb') as f:\n    data_basic = pickle.load(f)\ndf_basic = data_basic['df'] if isinstance(data_basic, dict) and 'df' in data_basic else data_basic\ndf_basic = df_basic.rename(columns={'Symbol': 'code', 'EndDate': 'year'})\ndf_basic['code'] = df_basic['code'].astype(str)\ndf_basic['year'] = df_basic['year'].astype(str)\n\nwith open(os.path.join(data_clean_folder, 'CSMAR常用变量-2000-2024.pkl'), 'rb') as f:\n    data_csmar = pickle.load(f)\ndf_csmar = data_csmar['df'] if isinstance(data_csmar, dict) and 'df' in data_csmar else data_csmar\ndf_csmar['code'] = df_csmar['Stkcd'].astype(str)\ndf_csmar['year'] = df_csmar['accper'].astype(str)\n\ndf_financial = merge_on_code_year(df_financial, df_basic, suffix='_basic')\ndf_final = merge_on_code_year(df_financial, df_csmar, suffix='_csmar')\n\n# 调整前两列顺序\ncols = list(df_final.columns)\nif 'code' in cols and 'year' in cols:\n    cols.remove('code')\n    cols.remove('year')\n    df_final = df_final[['code', 'year'] + cols]\n\n# 保存\nwith open(os.path.join(data_clean_folder, 'df_final.pkl'), 'wb') as f:\n    pickle.dump(df_final, f)\n\nprint(\"==== 合并后数据框 df_final ====\")\nprint(\"df_final shape:\", df_final.shape)\nprint(df_final.iloc[:3, :5])\nprint('-'*40)\n\n# 4. 合并数据字典\ndicts = []\n# 资产负债表\nwith open(os.path.join(data_clean_folder, '资产负债表-2011-2024.pkl'), 'rb') as f:\n    d = pickle.load(f)\n    if isinstance(d, dict) and 'varname_cn' in d:\n        dicts.append(d['varname_cn'])\n# 利润表\nfor fname in ['利润表-现金流量表-2000-2010.pkl', '利润表-现金流量表-2011-2024.pkl']:\n    pkl_path = os.path.join(data_clean_folder, fname)\n    if os.path.exists(pkl_path):\n        with open(pkl_path, 'rb') as f:\n            d = pickle.load(f)\n            if isinstance(d, dict) and 'varname_cn' in d:\n                dicts.append(d['varname_cn'])\n# 基本信息\nif isinstance(data_basic, dict) and 'varname_cn' in data_basic:\n    dicts.append(data_basic['varname_cn'])\n# CSMAR常用变量\nif isinstance(data_csmar, dict) and 'varname_cn' in data_csmar:\n    dicts.append(data_csmar['varname_cn'])\n\ndict_CSMAR = {}\nfor d in dicts:\n    dict_CSMAR.update(d)\n\nprint(\"==== 合并后数据字典 dict_CSMAR ====\")\nprint(f\"字典总变量数: {len(dict_CSMAR)}\")\nfor i, (k, v) in enumerate(dict_CSMAR.items()):\n    print(f\"{k}: {v}\")\n    if i &gt;= 200:\n        break\nprint('-'*40)\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[58], line 15\n     12     return merged\n     14 # 1. 资产负债表与利润表的合并\n---&gt; 15 with open(os.path.join(data_clean_folder, 'df_assets.pkl'), 'rb') as f:\n     16     df_assets = pickle.load(f)\n     17 with open(os.path.join(data_clean_folder, 'df_profit.pkl'), 'rb') as f:\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324, in _modified_open(file, *args, **kwargs)\n    317 if file in {0, 1, 2}:\n    318     raise ValueError(\n    319         f\"IPython won't let you open fd={file} by default \"\n    320         \"as it is likely to crash IPython. If you know what you are doing, \"\n    321         \"you can use builtins' open.\"\n    322     )\n--&gt; 324 return io_open(file, *args, **kwargs)\n\nFileNotFoundError: [Errno 2] No such file or directory: 'D:\\\\Github\\\\ds_data\\\\data\\\\CSMAR\\\\data_clean\\\\df_assets.pkl'\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# 只对数值型变量做统计\nnumeric_cols = df_final.select_dtypes(include=[np.number]).columns\n\n# 统计量\ndesc = df_final[numeric_cols].agg(['count', 'mean', 'std', 'min', 'max']).T\ndesc = desc.rename(columns={'count': 'N', 'mean': 'Mean', 'std': 'SD', 'min': 'Min', 'max': 'Max'})\n\n# 缺失值统计\nmissing = df_final.isnull().sum()\nmissing = missing[missing &gt; 0].sort_values(ascending=False)\n\nprint(\"==== 数值型变量基本统计量 (N, Mean, SD, Min, Max) ====\")\ndisplay(desc)\n\nprint(\"\\n==== 各变量缺失值数量 (只显示有缺失的变量) ====\")\ndisplay(missing)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[57], line 8\n      5 numeric_cols = df_final.select_dtypes(include=[np.number]).columns\n      7 # 统计量\n----&gt; 8 desc = df_final[numeric_cols].agg(['count', 'mean', 'std', 'min', 'max']).T\n      9 desc = desc.rename(columns={'count': 'N', 'mean': 'Mean', 'std': 'SD', 'min': 'Min', 'max': 'Max'})\n     11 # 缺失值统计\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10149, in DataFrame.aggregate(self, func, axis, *args, **kwargs)\n  10146 axis = self._get_axis_number(axis)\n  10148 op = frame_apply(self, func=func, axis=axis, args=args, kwargs=kwargs)\n&gt; 10149 result = op.agg()\n  10150 result = reconstruct_and_relabel_result(result, func, **kwargs)\n  10151 return result\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:928, in FrameApply.agg(self)\n    926 result = None\n    927 try:\n--&gt; 928     result = super().agg()\n    929 finally:\n    930     self.obj = obj\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:193, in Apply.agg(self)\n    190     return self.agg_dict_like()\n    191 elif is_list_like(func):\n    192     # we require a list, but not a 'str'\n--&gt; 193     return self.agg_list_like()\n    195 if callable(func):\n    196     f = com.get_cython_func(func)\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:326, in Apply.agg_list_like(self)\n    318 def agg_list_like(self) -&gt; DataFrame | Series:\n    319     \"\"\"\n    320     Compute aggregation in the case of a list-like argument.\n    321 \n   (...)\n    324     Result of aggregation.\n    325     \"\"\"\n--&gt; 326     return self.agg_or_apply_list_like(op_name=\"agg\")\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:745, in NDFrameApply.agg_or_apply_list_like(self, op_name)\n    742     raise NotImplementedError(\"axis other than 0 is not supported\")\n    744 keys, results = self.compute_list_like(op_name, obj, kwargs)\n--&gt; 745 result = self.wrap_results_list_like(keys, results)\n    746 return result\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:402, in Apply.wrap_results_list_like(self, keys, results)\n    399 obj = self.obj\n    401 try:\n--&gt; 402     return concat(results, keys=keys, axis=1, sort=False)\n    403 except TypeError as err:\n    404     # we are concatting non-NDFrame objects,\n    405     # e.g. a list of scalars\n    406     from pandas import Series\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382, in concat(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\n    379 elif copy and using_copy_on_write():\n    380     copy = False\n--&gt; 382 op = _Concatenator(\n    383     objs,\n    384     axis=axis,\n    385     ignore_index=ignore_index,\n    386     join=join,\n    387     keys=keys,\n    388     levels=levels,\n    389     names=names,\n    390     verify_integrity=verify_integrity,\n    391     copy=copy,\n    392     sort=sort,\n    393 )\n    395 return op.get_result()\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445, in _Concatenator.__init__(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\n    442 self.verify_integrity = verify_integrity\n    443 self.copy = copy\n--&gt; 445 objs, keys = self._clean_keys_and_objs(objs, keys)\n    447 # figure out what our result ndim is going to be\n    448 ndims = self._get_ndims(objs)\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507, in _Concatenator._clean_keys_and_objs(self, objs, keys)\n    504     objs_list = list(objs)\n    506 if len(objs_list) == 0:\n--&gt; 507     raise ValueError(\"No objects to concatenate\")\n    509 if keys is None:\n    510     objs_list = list(com.not_none(*objs_list))\n\nValueError: No objects to concatenate\n\n\n\n\ndf_final.summary()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n~\\AppData\\Local\\Temp\\ipykernel_53792\\2207892642.py in ?()\n----&gt; 1 df_final.summary()\n\nc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in ?(self, name)\n   6295             and name not in self._accessors\n   6296             and self._info_axis._can_hold_identifiers_and_holds_name(name)\n   6297         ):\n   6298             return self[name]\n-&gt; 6299         return object.__getattribute__(self, name)\n\nAttributeError: 'DataFrame' object has no attribute 'summary'\n\n\n\n\n\n13.9.3 输出最终数据文件\n\n\n\n\n\n\n提示词\n\n\n\n== 输出 csv 和 txt 文件到 [data_final] 文件夹\n\n将 df_final 数据框保存为 CSMAR_final.csv 文件，存储在 [data_final] 文件夹中。\n将 dict_CSMAR 字典保存为 CSMAR_var_label.txt 文件，存储在 [data_final] 文件夹中。每行格式为：变量名: 中文简称。\n\n\n\n\nimport os\n\n# 创建 data_final 文件夹（如果不存在）\ndata_final_folder = os.path.join(path, 'data_final')\nif not os.path.exists(data_final_folder):\n    os.makedirs(data_final_folder)\n\n# 1. 保存 df_final 为 CSV\ncsv_path = os.path.join(data_final_folder, 'CSMAR_final.csv')\ndf_final.to_csv(csv_path, index=False, encoding='utf-8-sig')\nprint(f\"已保存数据文件: {csv_path}\")\n\n# 2. 保存 dict_CSMAR 为 txt\ntxt_path = os.path.join(data_final_folder, 'CSMAR_var_label.txt')\nwith open(txt_path, 'w', encoding='utf-8') as f:\n    for k, v in dict_CSMAR.items():\n        f.write(f\"{k}: {v}\\n\")\nprint(f\"已保存变量标签文件: {txt_path}\")\n\n已保存数据文件: d:\\Github\\ds_data\\data\\CSMAR\\data_final\\CSMAR_final.csv\n已保存变量标签文件: d:\\Github\\ds_data\\data\\CSMAR\\data_final\\CSMAR_var_label.txt\n\n\n\n\n13.9.4 收尾：删除无用文件和过程文件\n\nNote：这一步是可选的，主要是为了清理不必要的文件，以节省存储空间和提高数据处理效率。\n建议：确认所有数据处理和合并工作完成后，再执行此步骤。\n\n有些过程文件已经不需要了：\n\ndata_raw 文件夹中的 .xlsx 文件已经转换为 .csv 文件，因此可以删除。\ndata_clean 文件夹中的一些文件也不需要了，如 利润表-现金流量表-xxx.pkl 和 资产负债表-xxx.pkl，因为它们已经被合并为 df_financial.pkl。\n\n\n\n\n\n\n\n提示词\n\n\n\n\n删除 data_raw 文件夹中所有子文件夹中的 .xlsx 文件。\n删除 data_clean 文件夹中如下文件：\n\n以 利润表-现金流量表- 开头的文件；\n以 资产负债表- 开头的文件；\n\n\n\n\n\nimport os\n\n# 1. 删除 data_raw 文件夹中所有子文件夹中的 .xlsx 文件\nfor subfolder in os.listdir(extract_folder):\n    subfolder_path = os.path.join(extract_folder, subfolder)\n    if os.path.isdir(subfolder_path):\n        for fname in os.listdir(subfolder_path):\n            if fname.endswith('.xlsx'):\n                file_path = os.path.join(subfolder_path, fname)\n                os.remove(file_path)\n                print(f\"已删除: {file_path}\")\n\n# 2. 删除 data_clean 文件夹中指定前缀的文件\nprefixes = ['利润表-现金流量表-', '资产负债表-']\nfor fname in os.listdir(data_clean_folder):\n    if any(fname.startswith(prefix) for prefix in prefixes):\n        file_path = os.path.join(data_clean_folder, fname)\n        if os.path.isfile(file_path):\n            os.remove(file_path)\n            print(f\"已删除: {file_path}\")\n\n\n\n13.9.5 呈现项目文档树\n要点：\n\n代码执行后会自动将工作目录切换回项目根目录，确保后续操作路径一致。\n\n\n\n\n\n\n\n提示词\n\n\n\n\n列示项目文档树结构。\n只列示文件夹名称和文件名称，不需要显示文件内容。\n如果文件夹中有子文件夹，则显示子文件夹名称。\n处理完后，将工作目录切换到项目根目录。\n为此代码块添加标题和合适的注释。\n\n\n\n\nimport os\n\n# === 项目文档树结构展示 ===\n# 本代码用于递归列示当前项目的文件夹和文件结构，便于快速了解项目目录布局。\n# 最后将工作目录切换回项目根目录。\n\ndef print_project_tree(root, indent=\"\"):\n    for item in os.listdir(root):\n        item_path = os.path.join(root, item)\n        print(indent + \"|-- \" + item)\n        if os.path.isdir(item_path):\n            print_project_tree(item_path, indent + \"    \")\n\nprint(\"=== 项目文档树结构 ===\")\nprint_project_tree(path)\n\n# 切换工作目录到项目根目录\nos.chdir(path)\n\n=== 项目文档树结构 ===\n|-- 01_read_merge.ipynb\n|-- 02_data_clean.ipynb\n|-- CSMAR_API.md\n|-- CSMAR_firm_basic_infor.ipynb\n|-- data_clean\n    |-- CSMAR常用变量-2000-2024.pkl\n    |-- df_assets.pkl\n    |-- df_final.pkl\n    |-- df_profit.pkl\n    |-- 上市公司基本信息变更表2000-2024.pkl\n    |-- 上市公司基本信息年度表.pkl\n    |-- 利润表-现金流量表-2000-2010.pkl\n    |-- 利润表-现金流量表-2011-2024.pkl\n    |-- 资产负债表-2000-2010.pkl\n    |-- 资产负债表-2011-2024.pkl\n|-- data_final\n    |-- CSMAR_final.csv\n    |-- CSMAR_var_label.txt\n|-- data_raw\n    |-- CSMAR常用变量-2000-2024\n        |-- CSMAR常用变量-2000-2024.csv\n        |-- CSMAR常用变量-2000-2024.xlsx\n        |-- CSMAR常用变量-2000-2024_DES.txt\n        |-- 版权声明.pdf\n    |-- 上市公司基本信息变更表2000-2024\n        |-- STK_LISTEDCOINFOCHG.csv\n        |-- STK_LISTEDCOINFOCHG.xlsx\n        |-- STK_LISTEDCOINFOCHG[DES][xlsx].txt\n        |-- 版权声明.pdf\n    |-- 上市公司基本信息年度表\n        |-- STK_LISTEDCOINFOANL.csv\n        |-- STK_LISTEDCOINFOANL.xlsx\n        |-- STK_LISTEDCOINFOANL[DES][xlsx].txt\n        |-- 上市公司基本信息 数据库说明书.pdf\n    |-- 利润表-现金流量表-2000-2010\n        |-- 利润表-现金流量表-2000-2010.csv\n        |-- 利润表-现金流量表-2000-2010.xlsx\n        |-- 利润表-现金流量表-2000-2010_DES.txt\n        |-- 版权声明.pdf\n    |-- 利润表-现金流量表-2011-2024\n        |-- 利润表-现金流量表-2011-2024.csv\n        |-- 利润表-现金流量表-2011-2024.xlsx\n        |-- 利润表-现金流量表-2011-2024_DES.txt\n        |-- 版权声明.pdf\n    |-- 资产负债表-2000-2010\n        |-- 版权声明.pdf\n        |-- 资产负债表-2000-2010.csv\n        |-- 资产负债表-2000-2010.xlsx\n        |-- 资产负债表-2000-2010_DES.txt\n    |-- 资产负债表-2011-2024\n        |-- 版权声明.pdf\n        |-- 资产负债表-2011-2024.csv\n        |-- 资产负债表-2011-2024.xlsx\n        |-- 资产负债表-2011-2024_DES.txt\n|-- data_raw_zip\n    |-- CSMAR常用变量-2000-2024.zip\n    |-- 上市公司基本信息变更表2000-2024.zip\n    |-- 上市公司基本信息年度表.zip\n    |-- 利润表-现金流量表-2000-2010.zip\n    |-- 利润表-现金流量表-2011-2024.zip\n    |-- 资产负债表-2000-2010.zip\n    |-- 资产负债表-2011-2024.zip\n|-- functions\n    |-- CSMAR_var_label.py\n\n\n\n\n13.9.6 保留哪些文件夹？\n在本地运行完毕后，项目根目录下会包含如下文件夹：\n\n[1] data_raw_zip 文件夹：务必保留，里面存放了从 CSMAR 下载的原始数据压缩包。\n[2] data_raw 文件夹：可以删除，里面存放了解压后的 .xlsx 和 .txt 文档\n[3] data_clean 文件夹：酌情删除，里面存放了清洗后的 .pkl 文件。\n[4] data_final 文件夹：建议保留，里面存放了最终的 .csv 和 .txt 文件。\n\n可复现文档分享方案：\n一旦确定上述代码无误，在分享时，只需保留 01_read_merge.ipynb 文件和 [data_raw_zip] 文件夹即可。\n\n完整复现文档：\n\n01_read_merge.ipynb 文件：包含了从数据读取到合并的完整代码。\n[data_raw_zip] 文件夹：包含了从 CSMAR 下载的原始数据压缩包。\n\n直接使用最终文档 + 了解处理过程：\n\ndata_final 文件夹：包含了最终的 .csv 和 .txt 文件，便于直接使用。\n01_read_merge.ipynb 文件：包含了从数据读取到合并的完整代码。\n\n\n\n## 下一步？\n\n接下来，我们会编写：\n\n-  `02_data_clean.ipynb` 文件，对 `data_final` 文件夹中的数据做进一步处理，包括：缺失值、文字变量转换为数值变量、生成新的变量、处理离群值等。\n-  `03_data_analysis.ipynb` 文件，对 `data_final` 文件夹中的数据进行分析，包括：描述性统计、相关性分析、回归分析等。",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html",
    "title": "14  案例：当当网 Python 类图书销售数据分析",
    "section": "",
    "text": "14.1 背景\n我想写一本有关 Python 应用的书。目前我有很多想法，拿不定主意。比如，\n在不了解这些信息的情况下，贸然画一年的时间写一本书是一个非常冒险的决定。为了降低风险，我决定先做一些市场调研，看看读者对 Python 书籍的需求。\n我先通过百度搜索和 Google 搜索，了解到当当网是中国最大的图书销售网站之一，拥有丰富的图书数据。我决定从当当网获取一些 Python 类图书的销售数据，进行分析，看看读者对 Python 书籍的需求。\n问题来了，我只有一些非常基础的 Python 知识 (大体上限于能对着课本或帮助文档提供的实例进行一些微小的修改，然后运行)。而且，我目前掌握的 Python 知识基本上进局限于回归分析。对于网络爬虫、数据分析、数据可视化等知识，我几乎一无所知。\n我听说 ChatGPT, DeepSeek 等 AI 工具可以帮助我完成一些复杂的任务，比如网络爬虫、数据分析、数据可视化等。我决定试试这些工具，看看它们能否帮助我完成我的市场调研。\n于是，我写一个提示词，发给了 ChatGPT：\n以下是 ChatGPT 的回复：",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#背景",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#背景",
    "title": "14  案例：当当网 Python 类图书销售数据分析",
    "section": "15.1 一、分析目标",
    "text": "目标读者是谁？经管类博士生，还是中小学生？\n书的内容是什么？是 Python 基础知识，还是 Python 数据分析？亦或是机器学习？\n书的形式是什么？是电子书，还是纸质书？亦或是两者都有？\n书的价格是多少？是 50 元，还是 100 元？亦或是 200 元？\n找哪家出版社？\n\n\n\n\n\n\n\n与 ChatGPT 的完整对话\n\n\n\n\n\n\n\n提示词\n\n\n\n- 目的：从当当网获取Python相关书籍的销售量信息，最终输出一份市场调研报告。\n- 语言：Python\n- 网站：[https://search.dangdang.com/](https://search.dangdang.com/)\n- 关键词：'python'\n- 排序关键词：'销量'\n- 排序方式：从高到低\n- 信息条数：前 20 条\n- 目前搜索结果页面URL：[https://search.dangdang.com/?key=python\\&act=input\\&sort\\_type=sort\\_sale\\_amt\\_desc#J\\_tab](https://search.dangdang.com/?key=python&act=input&sort_type=sort_sale_amt_desc#J_tab)\n\n- 预期输出： \n  - 书名，作者，年份，出版社，评论数，原价、折后价\n\n以上是我目前想到的。\n请帮我做一个全面的规划，包括分析目标、收集那些变量，分析流程，以及最终的报告形式、核心内容等。\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n好的，以下是一份较为系统的规划方案，适用于以 Python 编写程序从当当网采集“Python”相关书籍的销售量数据，并撰写一份结构完整的市场调研报告。\n\n\n15 Python 相关图书市场调研项目规划\n\n15.1 一、分析目标\n\n核心问题：当前在当当网上热销的 Python 图书有哪些？主要出版时间集中在哪些年份？哪些出版社主导了市场？\n业务用途：\n\n为出版商、培训机构、书店提供选书参考；\n了解当前市场偏好（入门/进阶、编程/数据分析/人工智能）；\n为课程教材选择提供数据支持。\n\n\n\n\n\n15.2 二、数据采集计划\n\n15.2.1 1. 数据来源\n\n网址：https://search.dangdang.com/?key=python&act=input&sort_type=sort_sale_amt_desc#J_tab\n请求类型：GET\n页数限制：目前仅采集排序后前 20 本书\n排序方式：按销量从高到低\n\n\n\n15.2.2 2. 待采集字段（建议）\n\n\n\n字段名\n说明\n\n\n\n\n书名\n完整书名\n\n\n作者\n主作者及其他合著者\n\n\n出版社\n出版机构\n\n\n出版时间\n年份或具体日期\n\n\n原价\n划线价\n\n\n折后价\n实际售价\n\n\n评论数\n用户评论数，反映活跃度\n\n\n星级评分\n若有星级则采集\n\n\n封面图链接\n便于后续可视化展示\n\n\n图书详情页链接\n为后续扩展字段或人工校验提供入口\n\n\n\n\n\n\n\n15.3 三、分析流程\n\n15.3.1 Step 1：数据采集（Python 编程）\n\n使用库：\n\nrequests 模拟访问\nBeautifulSoup 解析 HTML\nre 或 lxml 辅助提取字段\n\n反爬措施应对：\n\n合理设置 headers（如模拟浏览器 UA）\n增加 time.sleep() 随机延迟\n如遇 JavaScript 动态加载，可考虑 selenium\n\n\n\n\n15.3.2 Step 2：数据清洗与规范化\n\n作者、出版社等字段统一格式\n出版时间转为 datetime\n价格字段转换为 float\n评论数转换为整数（去除“条评论”字样）\n\n\n\n15.3.3 Step 3：数据分析\n\n15.3.3.1 3.1 描述性统计\n\n平均价格、评论数中位数\n出版年份分布（近几年为主）\n高频作者、出版社统计\n\n\n\n15.3.3.2 3.2 分类分析\n\n入门类 vs 进阶类\n数据分析 vs Web 编程 vs 机器学习\n是否为教材或考试参考书\n\n\n\n15.3.3.3 3.3 排行榜可视化\n\n前 10 图书销量/评论/价格对比柱状图\n出版时间折线图\n价格 vs 评论数散点图\n\n\n\n\n\n\n15.4 四、报告输出格式\n\n15.4.1 1. 技术报告形式（推荐）\n\n格式：Markdown / Quarto / PDF（可由 Jupyter Notebook 或 Quarto 生成）\n结构建议：\n\n# Python 图书销售市场调研报告（当当网）\n\n## 1. 项目背景与目标\n## 2. 数据采集与字段说明\n## 3. 市场总体概况\n   - 销售 Top20 图书一览\n   - 平均价格、评分、评论数等统计\n## 4. 出版特征分析\n   - 出版社分布、出版时间分布\n   - 作者贡献度分析\n## 5. 图书类型对比分析\n   - 入门、进阶分布\n   - 专业方向分布\n## 6. 结论与建议\n   - 当前趋势总结\n   - 出版或选书建议\n\n\n15.4.2 2. 附录部分\n\n原始数据表格（20 本书）\n数据采集代码（附说明）\n图形可视化汇总图（Matplotlib / Seaborn / Plotly）\n\n\n\n\n\n15.5 五、可扩展方向（可选）\n\n同步采集京东、淘宝图书频道以对比\n分析定价策略（原价 vs 折扣价规律）\n抽取关键词构建词云图\n构建图书推荐系统雏形\n\n\n如你同意，我可以继续为你编写：\n\nPython 抓取前 20 条当当网图书数据的爬虫脚本；\n可视化分析和 Markdown 报告生成模板；\n教学型讲义或课程材料用版本。\n\n是否现在开始写代码部分？是否使用 requests + BeautifulSoup，还是你希望使用 selenium 模拟浏览器？",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#一分析目标",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#一分析目标",
    "title": "14  案例：当当网 Python 类图书销售数据分析",
    "section": "",
    "text": "核心问题：当前在当当网上热销的 Python 图书有哪些？主要出版时间集中在哪些年份？哪些出版社主导了市场？\n业务用途：\n\n为出版商、培训机构、书店提供选书参考；\n了解当前市场偏好（入门/进阶、编程/数据分析/人工智能）；\n为课程教材选择提供数据支持。",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#二数据采集计划",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#二数据采集计划",
    "title": "14  案例：当当网 Python 类图书销售数据分析",
    "section": "15.2 二、数据采集计划",
    "text": "15.2 二、数据采集计划\n\n15.2.1 1. 数据来源\n\n网址：https://search.dangdang.com/?key=python&act=input&sort_type=sort_sale_amt_desc#J_tab\n请求类型：GET\n页数限制：目前仅采集排序后前 20 本书\n排序方式：按销量从高到低\n\n\n\n15.2.2 2. 待采集字段（建议）\n\n\n\n字段名\n说明\n\n\n\n\n书名\n完整书名\n\n\n作者\n主作者及其他合著者\n\n\n出版社\n出版机构\n\n\n出版时间\n年份或具体日期\n\n\n原价\n划线价\n\n\n折后价\n实际售价\n\n\n评论数\n用户评论数，反映活跃度\n\n\n星级评分\n若有星级则采集\n\n\n封面图链接\n便于后续可视化展示\n\n\n图书详情页链接\n为后续扩展字段或人工校验提供入口",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#三分析流程",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#三分析流程",
    "title": "14  案例：当当网 Python 类图书销售数据分析",
    "section": "15.3 三、分析流程",
    "text": "15.3 三、分析流程\n\n15.3.1 Step 1：数据采集（Python 编程）\n\n使用库：\n\nrequests 模拟访问\nBeautifulSoup 解析 HTML\nre 或 lxml 辅助提取字段\n\n反爬措施应对：\n\n合理设置 headers（如模拟浏览器 UA）\n增加 time.sleep() 随机延迟\n如遇 JavaScript 动态加载，可考虑 selenium\n\n\n\n\n15.3.2 Step 2：数据清洗与规范化\n\n作者、出版社等字段统一格式\n出版时间转为 datetime\n价格字段转换为 float\n评论数转换为整数（去除“条评论”字样）\n\n\n\n15.3.3 Step 3：数据分析\n\n15.3.3.1 3.1 描述性统计\n\n平均价格、评论数中位数\n出版年份分布（近几年为主）\n高频作者、出版社统计\n\n\n\n15.3.3.2 3.2 分类分析\n\n入门类 vs 进阶类\n数据分析 vs Web 编程 vs 机器学习\n是否为教材或考试参考书\n\n\n\n15.3.3.3 3.3 排行榜可视化\n\n前 10 图书销量/评论/价格对比柱状图\n出版时间折线图\n价格 vs 评论数散点图",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#四报告输出格式",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#四报告输出格式",
    "title": "14  案例：当当网 Python 类图书销售数据分析",
    "section": "15.4 四、报告输出格式",
    "text": "15.4 四、报告输出格式\n\n15.4.1 1. 技术报告形式（推荐）\n\n格式：Markdown / Quarto / PDF（可由 Jupyter Notebook 或 Quarto 生成）\n结构建议：\n\n# Python 图书销售市场调研报告（当当网）\n\n## 1. 项目背景与目标\n## 2. 数据采集与字段说明\n## 3. 市场总体概况\n   - 销售 Top20 图书一览\n   - 平均价格、评分、评论数等统计\n## 4. 出版特征分析\n   - 出版社分布、出版时间分布\n   - 作者贡献度分析\n## 5. 图书类型对比分析\n   - 入门、进阶分布\n   - 专业方向分布\n## 6. 结论与建议\n   - 当前趋势总结\n   - 出版或选书建议\n\n\n15.4.2 2. 附录部分\n\n原始数据表格（20 本书）\n数据采集代码（附说明）\n图形可视化汇总图（Matplotlib / Seaborn / Plotly）",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#五可扩展方向可选",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#五可扩展方向可选",
    "title": "14  案例：当当网 Python 类图书销售数据分析",
    "section": "15.5 五、可扩展方向（可选）",
    "text": "15.5 五、可扩展方向（可选）\n\n同步采集京东、淘宝图书频道以对比\n分析定价策略（原价 vs 折扣价规律）\n抽取关键词构建词云图\n构建图书推荐系统雏形\n\n\n如你同意，我可以继续为你编写：\n\nPython 抓取前 20 条当当网图书数据的爬虫脚本；\n可视化分析和 Markdown 报告生成模板；\n教学型讲义或课程材料用版本。\n\n是否现在开始写代码部分？是否使用 requests + BeautifulSoup，还是你希望使用 selenium 模拟浏览器？",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#爬取数据",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#爬取数据",
    "title": "14  案例：当当网 Python 类图书销售数据分析",
    "section": "15.6 爬取数据",
    "text": "15.6 爬取数据\n我们让 ChatGPT 执行第一步：\nStep 1：使用 Python 抓取当当网「Python」相关图书销量前 20 名信息 的完整采集代码。\n\n15.6.1 技术选择说明\n\n目标网址：https://search.dangdang.com/?key=python&sort_type=sort_sale_amt_desc\n方案：采用 requests + BeautifulSoup 进行静态页面解析（当当网图书搜索结果页大部分信息在 HTML 中，JS 渲染较少）\n爬取字段：\n\n书名\n作者\n出版社\n出版时间\n原价\n折后价\n评论数\n商品详情页链接\n\n\n\n\n15.6.2 Python 代码\n\n# 设置项目路径\n#   [./data]: 存储数据文件\n#   [./output]: 存储输出文件\n\n\n#---------------- 项目路径，酌情修改 -------------------\n#\npath = r'D:\\Github\\dslian\\cases\\web_dangdang_book' \n#\n#------------------------------------------------------\n\nimport os\nimport sys\npath_data = os.path.join(path, 'data')\npath_output = os.path.join(path, 'output')\n\nos.chdir(path)\nos.getcwd()\n\n\n'd:\\\\Github\\\\dslian\\\\cases\\\\web_dangdang_book'\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\nimport os\n\n# 设置目标搜索 URL（销量降序排序）\nurl = \"https://search.dangdang.com/?key=python&sort_type=sort_sale_amt_desc\"\n\n# 设置请求头，模拟浏览器访问\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \\\n                   (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n}\n\n# 发送请求并获取页面内容\nresponse = requests.get(url, headers=headers)\nresponse.encoding = \"gb2312\"\n\n# 使用 BeautifulSoup 解析 HTML 内容\nsoup = BeautifulSoup(response.text, \"html.parser\")\n\n# 所有图书的信息在 ul.bigimg li 中\nbook_list = soup.select(\"ul.bigimg &gt; li\")\n\n# 用于保存每本书的信息\nbooks_data = []\n\n# 遍历每本书的条目，提取信息\nfor item in book_list:\n    try:\n        title_tag = item.select_one(\"a[name='itemlist-title']\")\n        title = title_tag.get(\"title\").strip() if title_tag else \"\"\n        link = title_tag.get(\"href\") if title_tag else \"\"\n\n        author_tag = item.select_one(\"p.name a\")\n        author = author_tag.text.strip() if author_tag else \"\"\n\n        pub_info_tag = item.select_one(\"p.search_book_author\")\n        pub_info = pub_info_tag.text.strip() if pub_info_tag else \"\"\n        pub_info_parts = pub_info.split(\"\\xa0\\xa0\")\n\n        publisher = pub_info_parts[1].strip() if len(pub_info_parts) &gt; 1 else \"\"\n        pubdate = pub_info_parts[2].strip() if len(pub_info_parts) &gt; 2 else \"\"\n\n        comment_tag = item.select_one(\"p.search_star_line a\")\n        comment = comment_tag.text.strip() if comment_tag else \"\"\n\n        price_tag = item.select_one(\"p.price\")\n        orig_price_tag = price_tag.select_one(\"span.search_pre_price\") if price_tag else None\n        disc_price_tag = price_tag.select_one(\"span.search_now_price\") if price_tag else None\n        orig_price = orig_price_tag.text.strip(\"¥\") if orig_price_tag else \"\"\n        disc_price = disc_price_tag.text.strip(\"¥\") if disc_price_tag else \"\"\n\n        books_data.append({\n            \"书名\": title,\n            \"作者\": author,\n            \"出版社\": publisher,\n            \"出版时间\": pubdate,\n            \"原价\": orig_price,\n            \"折后价\": disc_price,\n            \"评论数\": comment,\n            \"详情页链接\": link\n        })\n\n        time.sleep(0.5)  # 加入短暂停顿防止访问过快\n\n    except Exception as e:\n        print(f\"解析失败：{e}\")\n        continue\n\n# 转换为 DataFrame\ndf_books = pd.DataFrame(books_data)\n\n# 确保保存路径存在\noutput_path = \"data\"\nos.makedirs(output_path, exist_ok=True)\noutput_file = os.path.join(output_path, \"dangdang_python_books_top20.csv\")\n\n# 保存为 CSV 文件\ndf_books.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n\n# 打印成功信息\nprint(f\"已成功下载 {len(df_books)} 条数据，\n        点击 [file://{os.path.abspath(output_file)}] \n        查看 dangdang_python_books_top20.csv 文件\")\n\n已成功下载 60 条数据，点击 [file://d:\\Github\\dslian\\cases\\web_dangdang_book\\data\\dangdang_python_books_top20.csv] 查看 dangdang_python_books_top20.csv 文件\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\nimport os\n\nurl = \"https://search.dangdang.com/?key=python&sort_type=sort_sale_amt_desc\"\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \\\n                   (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n}\n\nresponse = requests.get(url, headers=headers)\nresponse.encoding = \"gb2312\"\nsoup = BeautifulSoup(response.text, \"html.parser\")\nbook_list = soup.select(\"ul.bigimg &gt; li\")\n\nbooks_data = []\n\nfor item in book_list:\n    try:\n        # 1. 主标题和简介\n        title_tag = item.select_one(\"a[name='itemlist-title']\")\n        full_title = title_tag.get(\"title\").strip() if title_tag else \"\"\n        if \"：\" in full_title:\n            title, brief = full_title.split(\"：\", 1)\n        elif \"（\" in full_title:\n            title, brief = full_title.split(\"（\", 1)\n            brief = \"（\" + brief\n        else:\n            title, brief = full_title, \"\"\n\n        # 2. 详情页链接\n        link = title_tag.get(\"href\") if title_tag else \"\"\n\n        # 3. 作者 / 出版社 / 出版时间\n        pub_info_tag = item.select_one(\"p.search_book_author\")\n        pub_info_parts = pub_info_tag.text.strip().split(\"\\xa0\\xa0\") if pub_info_tag else []\n        author = pub_info_parts[0].replace(\"作者\", \"\").strip() if len(pub_info_parts) &gt; 0 else \"\"\n        publisher = pub_info_parts[1].strip() if len(pub_info_parts) &gt; 1 else \"\"\n        pubdate = pub_info_parts[2].strip() if len(pub_info_parts) &gt; 2 else \"\"\n\n        # 4. 评论数\n        comment_tag = item.select_one(\"p.search_star_line a\")\n        comment = comment_tag.text.strip() if comment_tag else \"\"\n\n        # 5. 价格\n        price_tag = item.select_one(\"p.price\")\n        orig_price_tag = price_tag.select_one(\"span.search_pre_price\") if price_tag else None\n        disc_price_tag = price_tag.select_one(\"span.search_now_price\") if price_tag else None\n        orig_price = orig_price_tag.text.strip(\"¥\") if orig_price_tag else \"\"\n        disc_price = disc_price_tag.text.strip(\"¥\") if disc_price_tag else \"\"\n\n        books_data.append({\n            \"书名\": title,\n            \"简介\": brief,\n            \"作者\": author,\n            \"出版社\": publisher,\n            \"出版时间\": pubdate,\n            \"原价\": orig_price,\n            \"折后价\": disc_price,\n            \"评论数\": comment,\n            \"详情页链接\": link\n        })\n\n        time.sleep(0.5)  # 加入短暂停顿防止访问过快\n\n    except Exception as e:\n        print(f\"解析失败：{e}\")\n        continue\n\ndf_books = pd.DataFrame(books_data)\n\noutput_path = \"data\"\nos.makedirs(output_path, exist_ok=True)\noutput_file = os.path.join(output_path, \"dangdang_python_books_cleaned.csv\")\ndf_books.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n\nprint(f\"已成功下载 {len(df_books)} 条数据，点击 [file://{os.path.abspath(output_file)}] 查看 dangdang_python_books_cleaned.csv 文件\")\n\n已成功下载 60 条数据，点击 [file://d:\\Github\\dslian\\cases\\web_dangdang_book\\data\\dangdang_python_books_cleaned.csv] 查看 dangdang_python_books_cleaned.csv 文件\n\n\n\n# 读入本地数据\ndf_books = pd.read_csv(r\"data\\dangdang_python_books_cleaned.csv\", encoding=\"utf-8-sig\")\n\n# 展示前几行结果\nprint(df_books.columns)\nprint(df_books.head(3))\nprint(df_books.info())\n\nIndex(['书名', '简介', '作者', '出版社', '出版时间', '原价', '折后价', '评论数', '详情页链接'], dtype='object')\n                                                  书名  \\\n0                                     Python编程三剑客第3版   \n1                                      小学生Python创意编程   \n2  Python编程从入门到实践 第3版 Python编程入门经典，自学利器，数据分析、网络爬虫...   \n\n                                                  简介  \\\n0          Python编程从入门到实践第3版+快速上手第2版+极客项目编程（当当套装共3册）   \n1  （视频教学版） 本书按照入门开发者的思维方式编写，非常适合孩子学习Python编程的基础知识...   \n2                                                NaN   \n\n                                                  作者  出版社  出版时间     原价   折后价  \\\n0                                            加入购物车收藏  NaN   NaN  267.8  21.6   \n1              刘凤飞 /2024-01-01  /清华大学出版社加入购物车购买电子书收藏  NaN   NaN   89.0  45.2   \n2  [美]埃里克・马瑟斯（Eric Matthes） /2023-05-01  /人民邮电出版社...  NaN   NaN  109.8  69.8   \n\n         评论数                                    详情页链接  \n0  124999条评论  //product.dangdang.com/12100010244.html  \n1    7157条评论     //product.dangdang.com/29666296.html  \n2   19970条评论     //product.dangdang.com/29564789.html  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 60 entries, 0 to 59\nData columns (total 9 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   书名      60 non-null     object \n 1   简介      29 non-null     object \n 2   作者      60 non-null     object \n 3   出版社     0 non-null      float64\n 4   出版时间    0 non-null      float64\n 5   原价      57 non-null     float64\n 6   折后价     60 non-null     float64\n 7   评论数     60 non-null     object \n 8   详情页链接   60 non-null     object \ndtypes: float64(4), object(5)\nmemory usage: 4.3+ KB\nNone",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#后续",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#后续",
    "title": "14  案例：当当网 Python 类图书销售数据分析",
    "section": "15.7 后续",
    "text": "15.7 后续\n我花了 30 分钟做到目前的样子，基本上搞懂了使用 Copilot 编写 Python 代码实现爬虫的流程。\n刚好当时在教授「数据分析与 Python 应用」的课程，我索性把这个案例作为课堂作业，要求学生们完成。\n\n作业要求：T1-当当网Python类图书销售分析\n学生提交的 Jupyter Notebook：\n\n当当网-G7-01：爬虫和数据清洗\n当当网-G7-02：数据分析",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "body/references.html",
    "href": "body/references.html",
    "title": "15  参考文献",
    "section": "",
    "text": "15.1 Papers",
    "crumbs": [
      "**参考文献和附录**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>参考文献</span>"
    ]
  },
  {
    "objectID": "body/references.html#papers",
    "href": "body/references.html#papers",
    "title": "15  参考文献",
    "section": "",
    "text": "Dong, M. M., Stratopoulos, T. C., & Wang, V. X. (2024). A scoping review of ChatGPT research in accounting and finance. International Journal of Accounting Information Systems, 55, 100715. Link, PDF, Google.\nGiesecke, O. (2024). AI at the Frontier of Economic Research. SSRN Electronic Journal. Link, -PDF-, Google.\nKorinek, A. (2023). Generative AI for Economic Research: Use Cases and Implications for Economists. Journal of Economic Literature, 61(4), 1281–1317. Link (rep), PDF, Appendix, Google. 作者主页-Tips-AI\n\n更新 1：2024 版全文更新\n更新 2：提供了很多 Prompts\n\nRenkema, M., & Tursunbayeva, A. (2024). The future of work of academics in the age of Artificial Intelligence: State-of-the-art and a research roadmap. Futures, 163, 103453. Link, PDF, Google.\nQin X, Huang M, Ding J. AITurk: Using ChatGPT for Social Science Research[J]. Available at SSRN 4922861, 2024. -PDF-. 中文推文, ATTurk",
    "crumbs": [
      "**参考文献和附录**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>参考文献</span>"
    ]
  },
  {
    "objectID": "body/references.html#相关推文",
    "href": "body/references.html#相关推文",
    "title": "15  参考文献",
    "section": "15.2 相关推文",
    "text": "15.2 相关推文\n\nChatGPT, 2024, 知乎热议：导师都是段子手.\n于凡, 2024, AI可以编写Stata代码吗？.\n任缘, 2024, xtnumfac：估计面板数据中共同因子的个数.\n余坚, 2023, Stata：ChatGPT你能帮我干点啥？.\n吴小齐, 2024, 强大的Kimi：中国版ChatGPT平替.\n吴欣洋, 2025, AI自动生成研究假设，靠谱吗？流程与挑战.\n张弛, 2025, 大语言模型到底是个啥？通俗易懂教程.\n张弛, 2025, 找不到IV？如何借助大语言模型寻找工具变量.\n张琪琳, 2025, CClaRA-扒了四万篇论文：如何论证因果关系？.\n张迪, 2022, Stata：AIC 和 BIC 准则介绍-aic_model_selection-stepwise.\n王文韬, 2021, ebalance：基于熵平衡法的协变量平衡性检验.\n王烨文, 2025, LLM Agent：大语言模型的智能体图解.\n王烨文, 2025, Marker：高效 PDF 文档解析与结构化提取工具.\n王烨文, 2025, 新书免费读：CausalMLBook-因果机器学习.\n罗丹, 2025, 提示词！提示词！数据清洗、数据分析、可视化一网打尽.\n罗兰若, 2022, Stata论文复现：高维线性回归的变量筛选-baing-ocmt.\n罗银燕, 2023, 如何在 R 中安装并使用 chatgpt 包？.\n苗妙, 2021, Stata绘图模板-极简风格：plotplain和plottig.\n董思源, 2025, 提示词！用 DeepSeek 快速生成更优代码.\n袁明生, 2025, 给经济学研究者的Prompt Engineering指南：如何用AI高效构建研究假设？.\n贺旭, 2020, Stata检验：AIC-BIC-MSE-MAE-等信息准则的计算.\n赵文琦, 2025, LLM系列：ChatGPT提示词精选与实操指南.\n连享会, 2024, AI编程助手大盘点：不止ChatGPT和Copilot.\n连享会, 2025, 连玉君老师公开课：AI时代如何做实证研究？.\n连小白, 2025, AI助手系列：napkin.ai-文字转换为图片和表格.\n连小白, 2025, AI助手系列：借助AI工具复现高质量图形.\n连小白, 2025, AI工具系列：英文学术论文语法检测与纠正.\n连小白, 2025, AI工具！AI工具分类大集合.\n连小白, 2025, No Chinglish：学术写作中的中式英语陷阱.\n连小白, 2025, 完美替代 MathPix：我用 ChatGPT、豆包、Kimi 识别数学公式.\n连小白, 2025, 完美替代 MathPix：我用 ChatGPT、豆包、Kimi 识别数学公式.\n连小白, 2025, 提示词来了！如何让 AI 翻译看起来像你写的.\n连小白, 2025, 自动化因果推断助手：Causal-Copilot 简介.\n连玉君, 2020, Sai2: 一款流畅的电子板书软件.\n连玉君, 2021, Stata安装路径中不要包含中文字符和空格：stacktrace not available错误信息.\n连玉君, 2021, Stata错误信息-stacktrace not available：安装路径中不要包含中文字符和空格.\n连玉君, 2024, VScode：实用 Markdown 插件推荐.\n连玉君, 2024, 从基础到 AI 助手：Python 用户最爱的 VScode 插件清单.\n连玉君, 2024, 借助ChatGPT4o学习排序算法：AI代码助手好酸爽！.\n连玉君, 2025, 如何借助 AI 工具来伴读一篇理论类的论文？.\n连玉君, 2025, 老连买电脑：ChatGPT，DeepSeek，豆包来帮忙.\n连玉君, 2023, 连玉君：我与ChatGPT聊了一个月.\n邱一崎, 2025, AITurk 平台论文复现.\n陈云菲, 2025, PDF神器MinerU：结构重构、图表提取、LaTeX公式识别全搞定！.\n陈庭伟, 2025, 2025年学术研究中的15大最佳AI工具.\n颜国强, 2024, ChatGPT争议：学术研究的加速器or信任危机的催化剂？.",
    "crumbs": [
      "**参考文献和附录**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>参考文献</span>"
    ]
  }
]